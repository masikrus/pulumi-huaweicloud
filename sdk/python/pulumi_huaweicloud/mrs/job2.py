# coding=utf-8
# *** WARNING: this file was generated by pulumi-language-python. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import builtins as _builtins
import warnings
import sys
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
if sys.version_info >= (3, 11):
    from typing import NotRequired, TypedDict, TypeAlias
else:
    from typing_extensions import NotRequired, TypedDict, TypeAlias
from .. import _utilities

__all__ = ['Job2Args', 'Job2']

@pulumi.input_type
class Job2Args:
    def __init__(__self__, *,
                 cluster_id: pulumi.Input[_builtins.str],
                 jar_path: pulumi.Input[_builtins.str],
                 job_name: pulumi.Input[_builtins.str],
                 job_type: pulumi.Input[_builtins.int],
                 arguments: Optional[pulumi.Input[_builtins.str]] = None,
                 hive_script_path: Optional[pulumi.Input[_builtins.str]] = None,
                 input: Optional[pulumi.Input[_builtins.str]] = None,
                 is_protected: Optional[pulumi.Input[_builtins.bool]] = None,
                 is_public: Optional[pulumi.Input[_builtins.bool]] = None,
                 job_log: Optional[pulumi.Input[_builtins.str]] = None,
                 output: Optional[pulumi.Input[_builtins.str]] = None,
                 region: Optional[pulumi.Input[_builtins.str]] = None):
        """
        The set of arguments for constructing a Job2 resource.
        :param pulumi.Input[_builtins.str] cluster_id: Cluster ID
        :param pulumi.Input[_builtins.str] jar_path: Path of the .jar package or .sql file for program execution The parameter
               must meet the following requirements: Contains a maximum of `1,023` characters, excluding special characters such as
               ;|&><'$. The address cannot be empty or full of spaces. Starts with / or s3a://. Spark Script must end with .sql;
               while MapReduce and Spark Jar must end with .jar. sql and jar are case-insensitive.
        :param pulumi.Input[_builtins.str] job_name: Job name Contains only `1` to `64` letters, digits, hyphens
               (-), and underscores (_). NOTE: Identical job names are allowed but not recommended.
        :param pulumi.Input[_builtins.int] job_type: Job type.
               + **1**: MapReduce
               + **2**: Spark
               + **3**: Hive Script
               + **4**: HiveQL (not supported currently)
               + **5**: DistCp, importing and exporting data
               + **6**: Spark Script
               + **7**: Spark SQL, submitting Spark SQL statements (not supported in this API currently).
               
               > NOTE: Spark and Hive jobs can be added to only clusters including Spark and Hive components.
        :param pulumi.Input[_builtins.str] arguments: Key parameter for program execution. The parameter is specified by the function of
               the user's program. MRS is only responsible for loading the parameter. The parameter contains a maximum of 2047
               characters, excluding special characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] hive_script_path: SQL program path This parameter is needed by Spark Script and Hive Script jobs
               only and must meet the following requirements:
               Contains a maximum of 1023 characters, excluding special characters such as ;|&><'$. The address cannot be empty or
               full of spaces. Starts with / or s3a://. Ends with .sql. sql is case-insensitive.
        :param pulumi.Input[_builtins.str] input: Path for inputting data, which must start with / or s3a://. A correct OBS path is
               required. The parameter contains a maximum of `1,023` characters, excluding special characters such as ;|&>'<$, and can
               be empty.
        :param pulumi.Input[_builtins.bool] is_protected: Whether a job is protected true false The current version does not support this
               function.
        :param pulumi.Input[_builtins.bool] is_public: Whether a job is public true false The current version does not support this function.
        :param pulumi.Input[_builtins.str] job_log: Path for storing job logs that record job running status. This path must start with /
               or s3a://. A correct OBS path is required. The parameter contains a maximum of 1023 characters, excluding special
               characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] output: Path for outputting data, which must start with / or s3a://. A correct OBS path is
               required. If the path does not exist, the system automatically creates it. The parameter contains a maximum of 1023
               characters, excluding special characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] region: The region in which to create the mrs job resource. If omitted, the
               provider-level region will be used. Changing this creates a new mrs job resource.
        """
        pulumi.set(__self__, "cluster_id", cluster_id)
        pulumi.set(__self__, "jar_path", jar_path)
        pulumi.set(__self__, "job_name", job_name)
        pulumi.set(__self__, "job_type", job_type)
        if arguments is not None:
            pulumi.set(__self__, "arguments", arguments)
        if hive_script_path is not None:
            pulumi.set(__self__, "hive_script_path", hive_script_path)
        if input is not None:
            pulumi.set(__self__, "input", input)
        if is_protected is not None:
            pulumi.set(__self__, "is_protected", is_protected)
        if is_public is not None:
            pulumi.set(__self__, "is_public", is_public)
        if job_log is not None:
            pulumi.set(__self__, "job_log", job_log)
        if output is not None:
            pulumi.set(__self__, "output", output)
        if region is not None:
            pulumi.set(__self__, "region", region)

    @_builtins.property
    @pulumi.getter(name="clusterId")
    def cluster_id(self) -> pulumi.Input[_builtins.str]:
        """
        Cluster ID
        """
        return pulumi.get(self, "cluster_id")

    @cluster_id.setter
    def cluster_id(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "cluster_id", value)

    @_builtins.property
    @pulumi.getter(name="jarPath")
    def jar_path(self) -> pulumi.Input[_builtins.str]:
        """
        Path of the .jar package or .sql file for program execution The parameter
        must meet the following requirements: Contains a maximum of `1,023` characters, excluding special characters such as
        ;|&><'$. The address cannot be empty or full of spaces. Starts with / or s3a://. Spark Script must end with .sql;
        while MapReduce and Spark Jar must end with .jar. sql and jar are case-insensitive.
        """
        return pulumi.get(self, "jar_path")

    @jar_path.setter
    def jar_path(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "jar_path", value)

    @_builtins.property
    @pulumi.getter(name="jobName")
    def job_name(self) -> pulumi.Input[_builtins.str]:
        """
        Job name Contains only `1` to `64` letters, digits, hyphens
        (-), and underscores (_). NOTE: Identical job names are allowed but not recommended.
        """
        return pulumi.get(self, "job_name")

    @job_name.setter
    def job_name(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "job_name", value)

    @_builtins.property
    @pulumi.getter(name="jobType")
    def job_type(self) -> pulumi.Input[_builtins.int]:
        """
        Job type.
        + **1**: MapReduce
        + **2**: Spark
        + **3**: Hive Script
        + **4**: HiveQL (not supported currently)
        + **5**: DistCp, importing and exporting data
        + **6**: Spark Script
        + **7**: Spark SQL, submitting Spark SQL statements (not supported in this API currently).

        > NOTE: Spark and Hive jobs can be added to only clusters including Spark and Hive components.
        """
        return pulumi.get(self, "job_type")

    @job_type.setter
    def job_type(self, value: pulumi.Input[_builtins.int]):
        pulumi.set(self, "job_type", value)

    @_builtins.property
    @pulumi.getter
    def arguments(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Key parameter for program execution. The parameter is specified by the function of
        the user's program. MRS is only responsible for loading the parameter. The parameter contains a maximum of 2047
        characters, excluding special characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "arguments")

    @arguments.setter
    def arguments(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "arguments", value)

    @_builtins.property
    @pulumi.getter(name="hiveScriptPath")
    def hive_script_path(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        SQL program path This parameter is needed by Spark Script and Hive Script jobs
        only and must meet the following requirements:
        Contains a maximum of 1023 characters, excluding special characters such as ;|&><'$. The address cannot be empty or
        full of spaces. Starts with / or s3a://. Ends with .sql. sql is case-insensitive.
        """
        return pulumi.get(self, "hive_script_path")

    @hive_script_path.setter
    def hive_script_path(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "hive_script_path", value)

    @_builtins.property
    @pulumi.getter
    def input(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Path for inputting data, which must start with / or s3a://. A correct OBS path is
        required. The parameter contains a maximum of `1,023` characters, excluding special characters such as ;|&>'<$, and can
        be empty.
        """
        return pulumi.get(self, "input")

    @input.setter
    def input(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "input", value)

    @_builtins.property
    @pulumi.getter(name="isProtected")
    def is_protected(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        Whether a job is protected true false The current version does not support this
        function.
        """
        return pulumi.get(self, "is_protected")

    @is_protected.setter
    def is_protected(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "is_protected", value)

    @_builtins.property
    @pulumi.getter(name="isPublic")
    def is_public(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        Whether a job is public true false The current version does not support this function.
        """
        return pulumi.get(self, "is_public")

    @is_public.setter
    def is_public(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "is_public", value)

    @_builtins.property
    @pulumi.getter(name="jobLog")
    def job_log(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Path for storing job logs that record job running status. This path must start with /
        or s3a://. A correct OBS path is required. The parameter contains a maximum of 1023 characters, excluding special
        characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "job_log")

    @job_log.setter
    def job_log(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "job_log", value)

    @_builtins.property
    @pulumi.getter
    def output(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Path for outputting data, which must start with / or s3a://. A correct OBS path is
        required. If the path does not exist, the system automatically creates it. The parameter contains a maximum of 1023
        characters, excluding special characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "output")

    @output.setter
    def output(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "output", value)

    @_builtins.property
    @pulumi.getter
    def region(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        The region in which to create the mrs job resource. If omitted, the
        provider-level region will be used. Changing this creates a new mrs job resource.
        """
        return pulumi.get(self, "region")

    @region.setter
    def region(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "region", value)


@pulumi.input_type
class _Job2State:
    def __init__(__self__, *,
                 arguments: Optional[pulumi.Input[_builtins.str]] = None,
                 cluster_id: Optional[pulumi.Input[_builtins.str]] = None,
                 hive_script_path: Optional[pulumi.Input[_builtins.str]] = None,
                 input: Optional[pulumi.Input[_builtins.str]] = None,
                 is_protected: Optional[pulumi.Input[_builtins.bool]] = None,
                 is_public: Optional[pulumi.Input[_builtins.bool]] = None,
                 jar_path: Optional[pulumi.Input[_builtins.str]] = None,
                 job_log: Optional[pulumi.Input[_builtins.str]] = None,
                 job_name: Optional[pulumi.Input[_builtins.str]] = None,
                 job_state: Optional[pulumi.Input[_builtins.str]] = None,
                 job_type: Optional[pulumi.Input[_builtins.int]] = None,
                 output: Optional[pulumi.Input[_builtins.str]] = None,
                 region: Optional[pulumi.Input[_builtins.str]] = None):
        """
        Input properties used for looking up and filtering Job2 resources.
        :param pulumi.Input[_builtins.str] arguments: Key parameter for program execution. The parameter is specified by the function of
               the user's program. MRS is only responsible for loading the parameter. The parameter contains a maximum of 2047
               characters, excluding special characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] cluster_id: Cluster ID
        :param pulumi.Input[_builtins.str] hive_script_path: SQL program path This parameter is needed by Spark Script and Hive Script jobs
               only and must meet the following requirements:
               Contains a maximum of 1023 characters, excluding special characters such as ;|&><'$. The address cannot be empty or
               full of spaces. Starts with / or s3a://. Ends with .sql. sql is case-insensitive.
        :param pulumi.Input[_builtins.str] input: Path for inputting data, which must start with / or s3a://. A correct OBS path is
               required. The parameter contains a maximum of `1,023` characters, excluding special characters such as ;|&>'<$, and can
               be empty.
        :param pulumi.Input[_builtins.bool] is_protected: Whether a job is protected true false The current version does not support this
               function.
        :param pulumi.Input[_builtins.bool] is_public: Whether a job is public true false The current version does not support this function.
        :param pulumi.Input[_builtins.str] jar_path: Path of the .jar package or .sql file for program execution The parameter
               must meet the following requirements: Contains a maximum of `1,023` characters, excluding special characters such as
               ;|&><'$. The address cannot be empty or full of spaces. Starts with / or s3a://. Spark Script must end with .sql;
               while MapReduce and Spark Jar must end with .jar. sql and jar are case-insensitive.
        :param pulumi.Input[_builtins.str] job_log: Path for storing job logs that record job running status. This path must start with /
               or s3a://. A correct OBS path is required. The parameter contains a maximum of 1023 characters, excluding special
               characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] job_name: Job name Contains only `1` to `64` letters, digits, hyphens
               (-), and underscores (_). NOTE: Identical job names are allowed but not recommended.
        :param pulumi.Input[_builtins.str] job_state: The resource job state.
        :param pulumi.Input[_builtins.int] job_type: Job type.
               + **1**: MapReduce
               + **2**: Spark
               + **3**: Hive Script
               + **4**: HiveQL (not supported currently)
               + **5**: DistCp, importing and exporting data
               + **6**: Spark Script
               + **7**: Spark SQL, submitting Spark SQL statements (not supported in this API currently).
               
               > NOTE: Spark and Hive jobs can be added to only clusters including Spark and Hive components.
        :param pulumi.Input[_builtins.str] output: Path for outputting data, which must start with / or s3a://. A correct OBS path is
               required. If the path does not exist, the system automatically creates it. The parameter contains a maximum of 1023
               characters, excluding special characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] region: The region in which to create the mrs job resource. If omitted, the
               provider-level region will be used. Changing this creates a new mrs job resource.
        """
        if arguments is not None:
            pulumi.set(__self__, "arguments", arguments)
        if cluster_id is not None:
            pulumi.set(__self__, "cluster_id", cluster_id)
        if hive_script_path is not None:
            pulumi.set(__self__, "hive_script_path", hive_script_path)
        if input is not None:
            pulumi.set(__self__, "input", input)
        if is_protected is not None:
            pulumi.set(__self__, "is_protected", is_protected)
        if is_public is not None:
            pulumi.set(__self__, "is_public", is_public)
        if jar_path is not None:
            pulumi.set(__self__, "jar_path", jar_path)
        if job_log is not None:
            pulumi.set(__self__, "job_log", job_log)
        if job_name is not None:
            pulumi.set(__self__, "job_name", job_name)
        if job_state is not None:
            pulumi.set(__self__, "job_state", job_state)
        if job_type is not None:
            pulumi.set(__self__, "job_type", job_type)
        if output is not None:
            pulumi.set(__self__, "output", output)
        if region is not None:
            pulumi.set(__self__, "region", region)

    @_builtins.property
    @pulumi.getter
    def arguments(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Key parameter for program execution. The parameter is specified by the function of
        the user's program. MRS is only responsible for loading the parameter. The parameter contains a maximum of 2047
        characters, excluding special characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "arguments")

    @arguments.setter
    def arguments(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "arguments", value)

    @_builtins.property
    @pulumi.getter(name="clusterId")
    def cluster_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Cluster ID
        """
        return pulumi.get(self, "cluster_id")

    @cluster_id.setter
    def cluster_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "cluster_id", value)

    @_builtins.property
    @pulumi.getter(name="hiveScriptPath")
    def hive_script_path(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        SQL program path This parameter is needed by Spark Script and Hive Script jobs
        only and must meet the following requirements:
        Contains a maximum of 1023 characters, excluding special characters such as ;|&><'$. The address cannot be empty or
        full of spaces. Starts with / or s3a://. Ends with .sql. sql is case-insensitive.
        """
        return pulumi.get(self, "hive_script_path")

    @hive_script_path.setter
    def hive_script_path(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "hive_script_path", value)

    @_builtins.property
    @pulumi.getter
    def input(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Path for inputting data, which must start with / or s3a://. A correct OBS path is
        required. The parameter contains a maximum of `1,023` characters, excluding special characters such as ;|&>'<$, and can
        be empty.
        """
        return pulumi.get(self, "input")

    @input.setter
    def input(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "input", value)

    @_builtins.property
    @pulumi.getter(name="isProtected")
    def is_protected(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        Whether a job is protected true false The current version does not support this
        function.
        """
        return pulumi.get(self, "is_protected")

    @is_protected.setter
    def is_protected(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "is_protected", value)

    @_builtins.property
    @pulumi.getter(name="isPublic")
    def is_public(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        Whether a job is public true false The current version does not support this function.
        """
        return pulumi.get(self, "is_public")

    @is_public.setter
    def is_public(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "is_public", value)

    @_builtins.property
    @pulumi.getter(name="jarPath")
    def jar_path(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Path of the .jar package or .sql file for program execution The parameter
        must meet the following requirements: Contains a maximum of `1,023` characters, excluding special characters such as
        ;|&><'$. The address cannot be empty or full of spaces. Starts with / or s3a://. Spark Script must end with .sql;
        while MapReduce and Spark Jar must end with .jar. sql and jar are case-insensitive.
        """
        return pulumi.get(self, "jar_path")

    @jar_path.setter
    def jar_path(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "jar_path", value)

    @_builtins.property
    @pulumi.getter(name="jobLog")
    def job_log(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Path for storing job logs that record job running status. This path must start with /
        or s3a://. A correct OBS path is required. The parameter contains a maximum of 1023 characters, excluding special
        characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "job_log")

    @job_log.setter
    def job_log(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "job_log", value)

    @_builtins.property
    @pulumi.getter(name="jobName")
    def job_name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Job name Contains only `1` to `64` letters, digits, hyphens
        (-), and underscores (_). NOTE: Identical job names are allowed but not recommended.
        """
        return pulumi.get(self, "job_name")

    @job_name.setter
    def job_name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "job_name", value)

    @_builtins.property
    @pulumi.getter(name="jobState")
    def job_state(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        The resource job state.
        """
        return pulumi.get(self, "job_state")

    @job_state.setter
    def job_state(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "job_state", value)

    @_builtins.property
    @pulumi.getter(name="jobType")
    def job_type(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Job type.
        + **1**: MapReduce
        + **2**: Spark
        + **3**: Hive Script
        + **4**: HiveQL (not supported currently)
        + **5**: DistCp, importing and exporting data
        + **6**: Spark Script
        + **7**: Spark SQL, submitting Spark SQL statements (not supported in this API currently).

        > NOTE: Spark and Hive jobs can be added to only clusters including Spark and Hive components.
        """
        return pulumi.get(self, "job_type")

    @job_type.setter
    def job_type(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "job_type", value)

    @_builtins.property
    @pulumi.getter
    def output(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Path for outputting data, which must start with / or s3a://. A correct OBS path is
        required. If the path does not exist, the system automatically creates it. The parameter contains a maximum of 1023
        characters, excluding special characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "output")

    @output.setter
    def output(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "output", value)

    @_builtins.property
    @pulumi.getter
    def region(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        The region in which to create the mrs job resource. If omitted, the
        provider-level region will be used. Changing this creates a new mrs job resource.
        """
        return pulumi.get(self, "region")

    @region.setter
    def region(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "region", value)


@pulumi.type_token("huaweicloud:Mrs/job2:Job2")
class Job2(pulumi.CustomResource):
    @overload
    def __init__(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 arguments: Optional[pulumi.Input[_builtins.str]] = None,
                 cluster_id: Optional[pulumi.Input[_builtins.str]] = None,
                 hive_script_path: Optional[pulumi.Input[_builtins.str]] = None,
                 input: Optional[pulumi.Input[_builtins.str]] = None,
                 is_protected: Optional[pulumi.Input[_builtins.bool]] = None,
                 is_public: Optional[pulumi.Input[_builtins.bool]] = None,
                 jar_path: Optional[pulumi.Input[_builtins.str]] = None,
                 job_log: Optional[pulumi.Input[_builtins.str]] = None,
                 job_name: Optional[pulumi.Input[_builtins.str]] = None,
                 job_type: Optional[pulumi.Input[_builtins.int]] = None,
                 output: Optional[pulumi.Input[_builtins.str]] = None,
                 region: Optional[pulumi.Input[_builtins.str]] = None,
                 __props__=None):
        """
        Manages resource job within HuaweiCloud MRS. It is recommend to use `Mrs.Job`, which makes a great
        improvement of managing MRS jobs.

        ## Example Usage

        ```python
        import pulumi
        import pulumi_huaweicloud as huaweicloud

        job1 = huaweicloud.mrs.Job2("job1",
            job_type=1,
            job_name="test_mapreduce_job1",
            cluster_id="ef43d2ff-1ecf-4f13-bd0c-0004c429a058",
            jar_path="s3a://wordcount/program/hadoop-mapreduce-examples-2.7.5.jar",
            input="s3a://wordcount/input/",
            output="s3a://wordcount/output/",
            job_log="s3a://wordcount/log/",
            arguments="wordcount")
        ```

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[_builtins.str] arguments: Key parameter for program execution. The parameter is specified by the function of
               the user's program. MRS is only responsible for loading the parameter. The parameter contains a maximum of 2047
               characters, excluding special characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] cluster_id: Cluster ID
        :param pulumi.Input[_builtins.str] hive_script_path: SQL program path This parameter is needed by Spark Script and Hive Script jobs
               only and must meet the following requirements:
               Contains a maximum of 1023 characters, excluding special characters such as ;|&><'$. The address cannot be empty or
               full of spaces. Starts with / or s3a://. Ends with .sql. sql is case-insensitive.
        :param pulumi.Input[_builtins.str] input: Path for inputting data, which must start with / or s3a://. A correct OBS path is
               required. The parameter contains a maximum of `1,023` characters, excluding special characters such as ;|&>'<$, and can
               be empty.
        :param pulumi.Input[_builtins.bool] is_protected: Whether a job is protected true false The current version does not support this
               function.
        :param pulumi.Input[_builtins.bool] is_public: Whether a job is public true false The current version does not support this function.
        :param pulumi.Input[_builtins.str] jar_path: Path of the .jar package or .sql file for program execution The parameter
               must meet the following requirements: Contains a maximum of `1,023` characters, excluding special characters such as
               ;|&><'$. The address cannot be empty or full of spaces. Starts with / or s3a://. Spark Script must end with .sql;
               while MapReduce and Spark Jar must end with .jar. sql and jar are case-insensitive.
        :param pulumi.Input[_builtins.str] job_log: Path for storing job logs that record job running status. This path must start with /
               or s3a://. A correct OBS path is required. The parameter contains a maximum of 1023 characters, excluding special
               characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] job_name: Job name Contains only `1` to `64` letters, digits, hyphens
               (-), and underscores (_). NOTE: Identical job names are allowed but not recommended.
        :param pulumi.Input[_builtins.int] job_type: Job type.
               + **1**: MapReduce
               + **2**: Spark
               + **3**: Hive Script
               + **4**: HiveQL (not supported currently)
               + **5**: DistCp, importing and exporting data
               + **6**: Spark Script
               + **7**: Spark SQL, submitting Spark SQL statements (not supported in this API currently).
               
               > NOTE: Spark and Hive jobs can be added to only clusters including Spark and Hive components.
        :param pulumi.Input[_builtins.str] output: Path for outputting data, which must start with / or s3a://. A correct OBS path is
               required. If the path does not exist, the system automatically creates it. The parameter contains a maximum of 1023
               characters, excluding special characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] region: The region in which to create the mrs job resource. If omitted, the
               provider-level region will be used. Changing this creates a new mrs job resource.
        """
        ...
    @overload
    def __init__(__self__,
                 resource_name: str,
                 args: Job2Args,
                 opts: Optional[pulumi.ResourceOptions] = None):
        """
        Manages resource job within HuaweiCloud MRS. It is recommend to use `Mrs.Job`, which makes a great
        improvement of managing MRS jobs.

        ## Example Usage

        ```python
        import pulumi
        import pulumi_huaweicloud as huaweicloud

        job1 = huaweicloud.mrs.Job2("job1",
            job_type=1,
            job_name="test_mapreduce_job1",
            cluster_id="ef43d2ff-1ecf-4f13-bd0c-0004c429a058",
            jar_path="s3a://wordcount/program/hadoop-mapreduce-examples-2.7.5.jar",
            input="s3a://wordcount/input/",
            output="s3a://wordcount/output/",
            job_log="s3a://wordcount/log/",
            arguments="wordcount")
        ```

        :param str resource_name: The name of the resource.
        :param Job2Args args: The arguments to use to populate this resource's properties.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        ...
    def __init__(__self__, resource_name: str, *args, **kwargs):
        resource_args, opts = _utilities.get_resource_args_opts(Job2Args, pulumi.ResourceOptions, *args, **kwargs)
        if resource_args is not None:
            __self__._internal_init(resource_name, opts, **resource_args.__dict__)
        else:
            __self__._internal_init(resource_name, *args, **kwargs)

    def _internal_init(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 arguments: Optional[pulumi.Input[_builtins.str]] = None,
                 cluster_id: Optional[pulumi.Input[_builtins.str]] = None,
                 hive_script_path: Optional[pulumi.Input[_builtins.str]] = None,
                 input: Optional[pulumi.Input[_builtins.str]] = None,
                 is_protected: Optional[pulumi.Input[_builtins.bool]] = None,
                 is_public: Optional[pulumi.Input[_builtins.bool]] = None,
                 jar_path: Optional[pulumi.Input[_builtins.str]] = None,
                 job_log: Optional[pulumi.Input[_builtins.str]] = None,
                 job_name: Optional[pulumi.Input[_builtins.str]] = None,
                 job_type: Optional[pulumi.Input[_builtins.int]] = None,
                 output: Optional[pulumi.Input[_builtins.str]] = None,
                 region: Optional[pulumi.Input[_builtins.str]] = None,
                 __props__=None):
        opts = pulumi.ResourceOptions.merge(_utilities.get_resource_opts_defaults(), opts)
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = Job2Args.__new__(Job2Args)

            __props__.__dict__["arguments"] = arguments
            if cluster_id is None and not opts.urn:
                raise TypeError("Missing required property 'cluster_id'")
            __props__.__dict__["cluster_id"] = cluster_id
            __props__.__dict__["hive_script_path"] = hive_script_path
            __props__.__dict__["input"] = input
            __props__.__dict__["is_protected"] = is_protected
            __props__.__dict__["is_public"] = is_public
            if jar_path is None and not opts.urn:
                raise TypeError("Missing required property 'jar_path'")
            __props__.__dict__["jar_path"] = jar_path
            __props__.__dict__["job_log"] = job_log
            if job_name is None and not opts.urn:
                raise TypeError("Missing required property 'job_name'")
            __props__.__dict__["job_name"] = job_name
            if job_type is None and not opts.urn:
                raise TypeError("Missing required property 'job_type'")
            __props__.__dict__["job_type"] = job_type
            __props__.__dict__["output"] = output
            __props__.__dict__["region"] = region
            __props__.__dict__["job_state"] = None
        super(Job2, __self__).__init__(
            'huaweicloud:Mrs/job2:Job2',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name: str,
            id: pulumi.Input[str],
            opts: Optional[pulumi.ResourceOptions] = None,
            arguments: Optional[pulumi.Input[_builtins.str]] = None,
            cluster_id: Optional[pulumi.Input[_builtins.str]] = None,
            hive_script_path: Optional[pulumi.Input[_builtins.str]] = None,
            input: Optional[pulumi.Input[_builtins.str]] = None,
            is_protected: Optional[pulumi.Input[_builtins.bool]] = None,
            is_public: Optional[pulumi.Input[_builtins.bool]] = None,
            jar_path: Optional[pulumi.Input[_builtins.str]] = None,
            job_log: Optional[pulumi.Input[_builtins.str]] = None,
            job_name: Optional[pulumi.Input[_builtins.str]] = None,
            job_state: Optional[pulumi.Input[_builtins.str]] = None,
            job_type: Optional[pulumi.Input[_builtins.int]] = None,
            output: Optional[pulumi.Input[_builtins.str]] = None,
            region: Optional[pulumi.Input[_builtins.str]] = None) -> 'Job2':
        """
        Get an existing Job2 resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param pulumi.Input[str] id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[_builtins.str] arguments: Key parameter for program execution. The parameter is specified by the function of
               the user's program. MRS is only responsible for loading the parameter. The parameter contains a maximum of 2047
               characters, excluding special characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] cluster_id: Cluster ID
        :param pulumi.Input[_builtins.str] hive_script_path: SQL program path This parameter is needed by Spark Script and Hive Script jobs
               only and must meet the following requirements:
               Contains a maximum of 1023 characters, excluding special characters such as ;|&><'$. The address cannot be empty or
               full of spaces. Starts with / or s3a://. Ends with .sql. sql is case-insensitive.
        :param pulumi.Input[_builtins.str] input: Path for inputting data, which must start with / or s3a://. A correct OBS path is
               required. The parameter contains a maximum of `1,023` characters, excluding special characters such as ;|&>'<$, and can
               be empty.
        :param pulumi.Input[_builtins.bool] is_protected: Whether a job is protected true false The current version does not support this
               function.
        :param pulumi.Input[_builtins.bool] is_public: Whether a job is public true false The current version does not support this function.
        :param pulumi.Input[_builtins.str] jar_path: Path of the .jar package or .sql file for program execution The parameter
               must meet the following requirements: Contains a maximum of `1,023` characters, excluding special characters such as
               ;|&><'$. The address cannot be empty or full of spaces. Starts with / or s3a://. Spark Script must end with .sql;
               while MapReduce and Spark Jar must end with .jar. sql and jar are case-insensitive.
        :param pulumi.Input[_builtins.str] job_log: Path for storing job logs that record job running status. This path must start with /
               or s3a://. A correct OBS path is required. The parameter contains a maximum of 1023 characters, excluding special
               characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] job_name: Job name Contains only `1` to `64` letters, digits, hyphens
               (-), and underscores (_). NOTE: Identical job names are allowed but not recommended.
        :param pulumi.Input[_builtins.str] job_state: The resource job state.
        :param pulumi.Input[_builtins.int] job_type: Job type.
               + **1**: MapReduce
               + **2**: Spark
               + **3**: Hive Script
               + **4**: HiveQL (not supported currently)
               + **5**: DistCp, importing and exporting data
               + **6**: Spark Script
               + **7**: Spark SQL, submitting Spark SQL statements (not supported in this API currently).
               
               > NOTE: Spark and Hive jobs can be added to only clusters including Spark and Hive components.
        :param pulumi.Input[_builtins.str] output: Path for outputting data, which must start with / or s3a://. A correct OBS path is
               required. If the path does not exist, the system automatically creates it. The parameter contains a maximum of 1023
               characters, excluding special characters such as ;|&>'<$, and can be empty.
        :param pulumi.Input[_builtins.str] region: The region in which to create the mrs job resource. If omitted, the
               provider-level region will be used. Changing this creates a new mrs job resource.
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = _Job2State.__new__(_Job2State)

        __props__.__dict__["arguments"] = arguments
        __props__.__dict__["cluster_id"] = cluster_id
        __props__.__dict__["hive_script_path"] = hive_script_path
        __props__.__dict__["input"] = input
        __props__.__dict__["is_protected"] = is_protected
        __props__.__dict__["is_public"] = is_public
        __props__.__dict__["jar_path"] = jar_path
        __props__.__dict__["job_log"] = job_log
        __props__.__dict__["job_name"] = job_name
        __props__.__dict__["job_state"] = job_state
        __props__.__dict__["job_type"] = job_type
        __props__.__dict__["output"] = output
        __props__.__dict__["region"] = region
        return Job2(resource_name, opts=opts, __props__=__props__)

    @_builtins.property
    @pulumi.getter
    def arguments(self) -> pulumi.Output[_builtins.str]:
        """
        Key parameter for program execution. The parameter is specified by the function of
        the user's program. MRS is only responsible for loading the parameter. The parameter contains a maximum of 2047
        characters, excluding special characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "arguments")

    @_builtins.property
    @pulumi.getter(name="clusterId")
    def cluster_id(self) -> pulumi.Output[_builtins.str]:
        """
        Cluster ID
        """
        return pulumi.get(self, "cluster_id")

    @_builtins.property
    @pulumi.getter(name="hiveScriptPath")
    def hive_script_path(self) -> pulumi.Output[_builtins.str]:
        """
        SQL program path This parameter is needed by Spark Script and Hive Script jobs
        only and must meet the following requirements:
        Contains a maximum of 1023 characters, excluding special characters such as ;|&><'$. The address cannot be empty or
        full of spaces. Starts with / or s3a://. Ends with .sql. sql is case-insensitive.
        """
        return pulumi.get(self, "hive_script_path")

    @_builtins.property
    @pulumi.getter
    def input(self) -> pulumi.Output[_builtins.str]:
        """
        Path for inputting data, which must start with / or s3a://. A correct OBS path is
        required. The parameter contains a maximum of `1,023` characters, excluding special characters such as ;|&>'<$, and can
        be empty.
        """
        return pulumi.get(self, "input")

    @_builtins.property
    @pulumi.getter(name="isProtected")
    def is_protected(self) -> pulumi.Output[_builtins.bool]:
        """
        Whether a job is protected true false The current version does not support this
        function.
        """
        return pulumi.get(self, "is_protected")

    @_builtins.property
    @pulumi.getter(name="isPublic")
    def is_public(self) -> pulumi.Output[_builtins.bool]:
        """
        Whether a job is public true false The current version does not support this function.
        """
        return pulumi.get(self, "is_public")

    @_builtins.property
    @pulumi.getter(name="jarPath")
    def jar_path(self) -> pulumi.Output[_builtins.str]:
        """
        Path of the .jar package or .sql file for program execution The parameter
        must meet the following requirements: Contains a maximum of `1,023` characters, excluding special characters such as
        ;|&><'$. The address cannot be empty or full of spaces. Starts with / or s3a://. Spark Script must end with .sql;
        while MapReduce and Spark Jar must end with .jar. sql and jar are case-insensitive.
        """
        return pulumi.get(self, "jar_path")

    @_builtins.property
    @pulumi.getter(name="jobLog")
    def job_log(self) -> pulumi.Output[_builtins.str]:
        """
        Path for storing job logs that record job running status. This path must start with /
        or s3a://. A correct OBS path is required. The parameter contains a maximum of 1023 characters, excluding special
        characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "job_log")

    @_builtins.property
    @pulumi.getter(name="jobName")
    def job_name(self) -> pulumi.Output[_builtins.str]:
        """
        Job name Contains only `1` to `64` letters, digits, hyphens
        (-), and underscores (_). NOTE: Identical job names are allowed but not recommended.
        """
        return pulumi.get(self, "job_name")

    @_builtins.property
    @pulumi.getter(name="jobState")
    def job_state(self) -> pulumi.Output[_builtins.str]:
        """
        The resource job state.
        """
        return pulumi.get(self, "job_state")

    @_builtins.property
    @pulumi.getter(name="jobType")
    def job_type(self) -> pulumi.Output[_builtins.int]:
        """
        Job type.
        + **1**: MapReduce
        + **2**: Spark
        + **3**: Hive Script
        + **4**: HiveQL (not supported currently)
        + **5**: DistCp, importing and exporting data
        + **6**: Spark Script
        + **7**: Spark SQL, submitting Spark SQL statements (not supported in this API currently).

        > NOTE: Spark and Hive jobs can be added to only clusters including Spark and Hive components.
        """
        return pulumi.get(self, "job_type")

    @_builtins.property
    @pulumi.getter
    def output(self) -> pulumi.Output[_builtins.str]:
        """
        Path for outputting data, which must start with / or s3a://. A correct OBS path is
        required. If the path does not exist, the system automatically creates it. The parameter contains a maximum of 1023
        characters, excluding special characters such as ;|&>'<$, and can be empty.
        """
        return pulumi.get(self, "output")

    @_builtins.property
    @pulumi.getter
    def region(self) -> pulumi.Output[_builtins.str]:
        """
        The region in which to create the mrs job resource. If omitted, the
        provider-level region will be used. Changing this creates a new mrs job resource.
        """
        return pulumi.get(self, "region")

