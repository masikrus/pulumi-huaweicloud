# coding=utf-8
# *** WARNING: this file was generated by pulumi-language-python. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import builtins as _builtins
import warnings
import sys
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
if sys.version_info >= (3, 11):
    from typing import NotRequired, TypedDict, TypeAlias
else:
    from typing_extensions import NotRequired, TypedDict, TypeAlias
from .. import _utilities

__all__ = [
    'DatasourceConnectionHostArgs',
    'DatasourceConnectionHostArgsDict',
    'DatasourceConnectionRouteArgs',
    'DatasourceConnectionRouteArgsDict',
    'QueueScalingPolicyArgs',
    'QueueScalingPolicyArgsDict',
    'QueueSparkDriverArgs',
    'QueueSparkDriverArgsDict',
    'QueueV1ScalingPolicyArgs',
    'QueueV1ScalingPolicyArgsDict',
    'QueueV1SparkDriverArgs',
    'QueueV1SparkDriverArgsDict',
    'SparkJobDependentPackageArgs',
    'SparkJobDependentPackageArgsDict',
    'SparkJobDependentPackagePackageArgs',
    'SparkJobDependentPackagePackageArgsDict',
    'SparkTemplateBodyArgs',
    'SparkTemplateBodyArgsDict',
    'SparkTemplateBodyDependentPackageArgs',
    'SparkTemplateBodyDependentPackageArgsDict',
    'SparkTemplateBodyDependentPackageResourceArgs',
    'SparkTemplateBodyDependentPackageResourceArgsDict',
    'SparkTemplateBodyResourceArgs',
    'SparkTemplateBodyResourceArgsDict',
    'SqlJobConfArgs',
    'SqlJobConfArgsDict',
    'TableColumnArgs',
    'TableColumnArgsDict',
    'TemplateSparkBodyArgs',
    'TemplateSparkBodyArgsDict',
    'TemplateSparkBodyDependentPackageArgs',
    'TemplateSparkBodyDependentPackageArgsDict',
    'TemplateSparkBodyDependentPackageResourceArgs',
    'TemplateSparkBodyDependentPackageResourceArgsDict',
    'TemplateSparkBodyResourceArgs',
    'TemplateSparkBodyResourceArgsDict',
]

MYPY = False

if not MYPY:
    class DatasourceConnectionHostArgsDict(TypedDict):
        ip: pulumi.Input[_builtins.str]
        """
        IPv4 address of the host.

        <a name="datasourceConnection_Route"></a>
        The `Route` block supports:
        """
        name: pulumi.Input[_builtins.str]
        """
        The route name.  
        The valid length is limited from `1` to `64`.
        """
elif False:
    DatasourceConnectionHostArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class DatasourceConnectionHostArgs:
    def __init__(__self__, *,
                 ip: pulumi.Input[_builtins.str],
                 name: pulumi.Input[_builtins.str]):
        """
        :param pulumi.Input[_builtins.str] ip: IPv4 address of the host.
               
               <a name="datasourceConnection_Route"></a>
               The `Route` block supports:
        :param pulumi.Input[_builtins.str] name: The route name.  
               The valid length is limited from `1` to `64`.
        """
        pulumi.set(__self__, "ip", ip)
        pulumi.set(__self__, "name", name)

    @_builtins.property
    @pulumi.getter
    def ip(self) -> pulumi.Input[_builtins.str]:
        """
        IPv4 address of the host.

        <a name="datasourceConnection_Route"></a>
        The `Route` block supports:
        """
        return pulumi.get(self, "ip")

    @ip.setter
    def ip(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "ip", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> pulumi.Input[_builtins.str]:
        """
        The route name.  
        The valid length is limited from `1` to `64`.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "name", value)


if not MYPY:
    class DatasourceConnectionRouteArgsDict(TypedDict):
        cidr: pulumi.Input[_builtins.str]
        """
        The CIDR of the route.
        """
        name: pulumi.Input[_builtins.str]
        """
        The route name.  
        The valid length is limited from `1` to `64`.
        """
elif False:
    DatasourceConnectionRouteArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class DatasourceConnectionRouteArgs:
    def __init__(__self__, *,
                 cidr: pulumi.Input[_builtins.str],
                 name: pulumi.Input[_builtins.str]):
        """
        :param pulumi.Input[_builtins.str] cidr: The CIDR of the route.
        :param pulumi.Input[_builtins.str] name: The route name.  
               The valid length is limited from `1` to `64`.
        """
        pulumi.set(__self__, "cidr", cidr)
        pulumi.set(__self__, "name", name)

    @_builtins.property
    @pulumi.getter
    def cidr(self) -> pulumi.Input[_builtins.str]:
        """
        The CIDR of the route.
        """
        return pulumi.get(self, "cidr")

    @cidr.setter
    def cidr(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "cidr", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> pulumi.Input[_builtins.str]:
        """
        The route name.  
        The valid length is limited from `1` to `64`.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "name", value)


if not MYPY:
    class QueueScalingPolicyArgsDict(TypedDict):
        impact_start_time: pulumi.Input[_builtins.str]
        """
        Specifies the effective time of the queue scaling policy.
        The value can be set only by hour.
        """
        impact_stop_time: pulumi.Input[_builtins.str]
        """
        Specifies the expiration time of the queue scaling policy.
        The value can be set only by hour.

        > The time ranges of different scaling policies in the same queue cannot overlap.
        The time range includes the start time but not the end time, e.g. `[00:00, 24:00)`.
        """
        max_cu: pulumi.Input[_builtins.int]
        """
        Specifies the maximum number of CUs allowed by the scaling policy.
        The number must be a multiple of `4`.

        > The maximum CUs of any queue in an elastic resource pool cannot be more than the maximum CUs of the pool.

        <a name="queue_spark_driver"></a>
        The `spark_driver` block supports:
        """
        min_cu: pulumi.Input[_builtins.int]
        """
        Specifies the minimum number of CUs allowed by the scaling policy.
        The number must be a multiple of `4`.

        > The total minimum CUs of all queues in an elastic resource pool cannot be more than the minimum CUs of the pool.
        """
        priority: pulumi.Input[_builtins.int]
        """
        Specifies the priority of the queue scaling policy.
        The valid value ranges from `1` to `100`. The larger value means the higher priority.
        """
elif False:
    QueueScalingPolicyArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class QueueScalingPolicyArgs:
    def __init__(__self__, *,
                 impact_start_time: pulumi.Input[_builtins.str],
                 impact_stop_time: pulumi.Input[_builtins.str],
                 max_cu: pulumi.Input[_builtins.int],
                 min_cu: pulumi.Input[_builtins.int],
                 priority: pulumi.Input[_builtins.int]):
        """
        :param pulumi.Input[_builtins.str] impact_start_time: Specifies the effective time of the queue scaling policy.
               The value can be set only by hour.
        :param pulumi.Input[_builtins.str] impact_stop_time: Specifies the expiration time of the queue scaling policy.
               The value can be set only by hour.
               
               > The time ranges of different scaling policies in the same queue cannot overlap.
               The time range includes the start time but not the end time, e.g. `[00:00, 24:00)`.
        :param pulumi.Input[_builtins.int] max_cu: Specifies the maximum number of CUs allowed by the scaling policy.
               The number must be a multiple of `4`.
               
               > The maximum CUs of any queue in an elastic resource pool cannot be more than the maximum CUs of the pool.
               
               <a name="queue_spark_driver"></a>
               The `spark_driver` block supports:
        :param pulumi.Input[_builtins.int] min_cu: Specifies the minimum number of CUs allowed by the scaling policy.
               The number must be a multiple of `4`.
               
               > The total minimum CUs of all queues in an elastic resource pool cannot be more than the minimum CUs of the pool.
        :param pulumi.Input[_builtins.int] priority: Specifies the priority of the queue scaling policy.
               The valid value ranges from `1` to `100`. The larger value means the higher priority.
        """
        pulumi.set(__self__, "impact_start_time", impact_start_time)
        pulumi.set(__self__, "impact_stop_time", impact_stop_time)
        pulumi.set(__self__, "max_cu", max_cu)
        pulumi.set(__self__, "min_cu", min_cu)
        pulumi.set(__self__, "priority", priority)

    @_builtins.property
    @pulumi.getter(name="impactStartTime")
    def impact_start_time(self) -> pulumi.Input[_builtins.str]:
        """
        Specifies the effective time of the queue scaling policy.
        The value can be set only by hour.
        """
        return pulumi.get(self, "impact_start_time")

    @impact_start_time.setter
    def impact_start_time(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "impact_start_time", value)

    @_builtins.property
    @pulumi.getter(name="impactStopTime")
    def impact_stop_time(self) -> pulumi.Input[_builtins.str]:
        """
        Specifies the expiration time of the queue scaling policy.
        The value can be set only by hour.

        > The time ranges of different scaling policies in the same queue cannot overlap.
        The time range includes the start time but not the end time, e.g. `[00:00, 24:00)`.
        """
        return pulumi.get(self, "impact_stop_time")

    @impact_stop_time.setter
    def impact_stop_time(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "impact_stop_time", value)

    @_builtins.property
    @pulumi.getter(name="maxCu")
    def max_cu(self) -> pulumi.Input[_builtins.int]:
        """
        Specifies the maximum number of CUs allowed by the scaling policy.
        The number must be a multiple of `4`.

        > The maximum CUs of any queue in an elastic resource pool cannot be more than the maximum CUs of the pool.

        <a name="queue_spark_driver"></a>
        The `spark_driver` block supports:
        """
        return pulumi.get(self, "max_cu")

    @max_cu.setter
    def max_cu(self, value: pulumi.Input[_builtins.int]):
        pulumi.set(self, "max_cu", value)

    @_builtins.property
    @pulumi.getter(name="minCu")
    def min_cu(self) -> pulumi.Input[_builtins.int]:
        """
        Specifies the minimum number of CUs allowed by the scaling policy.
        The number must be a multiple of `4`.

        > The total minimum CUs of all queues in an elastic resource pool cannot be more than the minimum CUs of the pool.
        """
        return pulumi.get(self, "min_cu")

    @min_cu.setter
    def min_cu(self, value: pulumi.Input[_builtins.int]):
        pulumi.set(self, "min_cu", value)

    @_builtins.property
    @pulumi.getter
    def priority(self) -> pulumi.Input[_builtins.int]:
        """
        Specifies the priority of the queue scaling policy.
        The valid value ranges from `1` to `100`. The larger value means the higher priority.
        """
        return pulumi.get(self, "priority")

    @priority.setter
    def priority(self, value: pulumi.Input[_builtins.int]):
        pulumi.set(self, "priority", value)


if not MYPY:
    class QueueSparkDriverArgsDict(TypedDict):
        max_concurrent: NotRequired[pulumi.Input[_builtins.int]]
        """
        Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
        The valid value ranges from `1` to `32`.
        """
        max_instance: NotRequired[pulumi.Input[_builtins.int]]
        """
        Specifies the maximum number of spark drivers that can be started on the queue.
        If the `cu_count` is `16`, the value can only be `2`.
        If The `cu_count` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
        divided by `16`.
        """
        max_prefetch_instance: NotRequired[pulumi.Input[_builtins.str]]
        """
        Specifies the maximum number of spark drivers to be pre-started on the queue.
        The minimum value is `0`. If the `cu_count` is less than `32`, the maximum value is `1`.
        If the `cu_count` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.

        > If the minimum CUs of the queue is less than `16` CUs, the `max_instance` and `max_prefetch_instance` parameters
        does not take effect.
        """
elif False:
    QueueSparkDriverArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class QueueSparkDriverArgs:
    def __init__(__self__, *,
                 max_concurrent: Optional[pulumi.Input[_builtins.int]] = None,
                 max_instance: Optional[pulumi.Input[_builtins.int]] = None,
                 max_prefetch_instance: Optional[pulumi.Input[_builtins.str]] = None):
        """
        :param pulumi.Input[_builtins.int] max_concurrent: Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
               The valid value ranges from `1` to `32`.
        :param pulumi.Input[_builtins.int] max_instance: Specifies the maximum number of spark drivers that can be started on the queue.
               If the `cu_count` is `16`, the value can only be `2`.
               If The `cu_count` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
               divided by `16`.
        :param pulumi.Input[_builtins.str] max_prefetch_instance: Specifies the maximum number of spark drivers to be pre-started on the queue.
               The minimum value is `0`. If the `cu_count` is less than `32`, the maximum value is `1`.
               If the `cu_count` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
               
               > If the minimum CUs of the queue is less than `16` CUs, the `max_instance` and `max_prefetch_instance` parameters
               does not take effect.
        """
        if max_concurrent is not None:
            pulumi.set(__self__, "max_concurrent", max_concurrent)
        if max_instance is not None:
            pulumi.set(__self__, "max_instance", max_instance)
        if max_prefetch_instance is not None:
            pulumi.set(__self__, "max_prefetch_instance", max_prefetch_instance)

    @_builtins.property
    @pulumi.getter(name="maxConcurrent")
    def max_concurrent(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
        The valid value ranges from `1` to `32`.
        """
        return pulumi.get(self, "max_concurrent")

    @max_concurrent.setter
    def max_concurrent(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "max_concurrent", value)

    @_builtins.property
    @pulumi.getter(name="maxInstance")
    def max_instance(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Specifies the maximum number of spark drivers that can be started on the queue.
        If the `cu_count` is `16`, the value can only be `2`.
        If The `cu_count` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
        divided by `16`.
        """
        return pulumi.get(self, "max_instance")

    @max_instance.setter
    def max_instance(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "max_instance", value)

    @_builtins.property
    @pulumi.getter(name="maxPrefetchInstance")
    def max_prefetch_instance(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Specifies the maximum number of spark drivers to be pre-started on the queue.
        The minimum value is `0`. If the `cu_count` is less than `32`, the maximum value is `1`.
        If the `cu_count` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.

        > If the minimum CUs of the queue is less than `16` CUs, the `max_instance` and `max_prefetch_instance` parameters
        does not take effect.
        """
        return pulumi.get(self, "max_prefetch_instance")

    @max_prefetch_instance.setter
    def max_prefetch_instance(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "max_prefetch_instance", value)


if not MYPY:
    class QueueV1ScalingPolicyArgsDict(TypedDict):
        impact_start_time: pulumi.Input[_builtins.str]
        impact_stop_time: pulumi.Input[_builtins.str]
        max_cu: pulumi.Input[_builtins.int]
        min_cu: pulumi.Input[_builtins.int]
        priority: pulumi.Input[_builtins.int]
elif False:
    QueueV1ScalingPolicyArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class QueueV1ScalingPolicyArgs:
    def __init__(__self__, *,
                 impact_start_time: pulumi.Input[_builtins.str],
                 impact_stop_time: pulumi.Input[_builtins.str],
                 max_cu: pulumi.Input[_builtins.int],
                 min_cu: pulumi.Input[_builtins.int],
                 priority: pulumi.Input[_builtins.int]):
        pulumi.set(__self__, "impact_start_time", impact_start_time)
        pulumi.set(__self__, "impact_stop_time", impact_stop_time)
        pulumi.set(__self__, "max_cu", max_cu)
        pulumi.set(__self__, "min_cu", min_cu)
        pulumi.set(__self__, "priority", priority)

    @_builtins.property
    @pulumi.getter(name="impactStartTime")
    def impact_start_time(self) -> pulumi.Input[_builtins.str]:
        return pulumi.get(self, "impact_start_time")

    @impact_start_time.setter
    def impact_start_time(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "impact_start_time", value)

    @_builtins.property
    @pulumi.getter(name="impactStopTime")
    def impact_stop_time(self) -> pulumi.Input[_builtins.str]:
        return pulumi.get(self, "impact_stop_time")

    @impact_stop_time.setter
    def impact_stop_time(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "impact_stop_time", value)

    @_builtins.property
    @pulumi.getter(name="maxCu")
    def max_cu(self) -> pulumi.Input[_builtins.int]:
        return pulumi.get(self, "max_cu")

    @max_cu.setter
    def max_cu(self, value: pulumi.Input[_builtins.int]):
        pulumi.set(self, "max_cu", value)

    @_builtins.property
    @pulumi.getter(name="minCu")
    def min_cu(self) -> pulumi.Input[_builtins.int]:
        return pulumi.get(self, "min_cu")

    @min_cu.setter
    def min_cu(self, value: pulumi.Input[_builtins.int]):
        pulumi.set(self, "min_cu", value)

    @_builtins.property
    @pulumi.getter
    def priority(self) -> pulumi.Input[_builtins.int]:
        return pulumi.get(self, "priority")

    @priority.setter
    def priority(self, value: pulumi.Input[_builtins.int]):
        pulumi.set(self, "priority", value)


if not MYPY:
    class QueueV1SparkDriverArgsDict(TypedDict):
        max_concurrent: NotRequired[pulumi.Input[_builtins.int]]
        max_instance: NotRequired[pulumi.Input[_builtins.int]]
        max_prefetch_instance: NotRequired[pulumi.Input[_builtins.str]]
elif False:
    QueueV1SparkDriverArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class QueueV1SparkDriverArgs:
    def __init__(__self__, *,
                 max_concurrent: Optional[pulumi.Input[_builtins.int]] = None,
                 max_instance: Optional[pulumi.Input[_builtins.int]] = None,
                 max_prefetch_instance: Optional[pulumi.Input[_builtins.str]] = None):
        if max_concurrent is not None:
            pulumi.set(__self__, "max_concurrent", max_concurrent)
        if max_instance is not None:
            pulumi.set(__self__, "max_instance", max_instance)
        if max_prefetch_instance is not None:
            pulumi.set(__self__, "max_prefetch_instance", max_prefetch_instance)

    @_builtins.property
    @pulumi.getter(name="maxConcurrent")
    def max_concurrent(self) -> Optional[pulumi.Input[_builtins.int]]:
        return pulumi.get(self, "max_concurrent")

    @max_concurrent.setter
    def max_concurrent(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "max_concurrent", value)

    @_builtins.property
    @pulumi.getter(name="maxInstance")
    def max_instance(self) -> Optional[pulumi.Input[_builtins.int]]:
        return pulumi.get(self, "max_instance")

    @max_instance.setter
    def max_instance(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "max_instance", value)

    @_builtins.property
    @pulumi.getter(name="maxPrefetchInstance")
    def max_prefetch_instance(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "max_prefetch_instance")

    @max_prefetch_instance.setter
    def max_prefetch_instance(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "max_prefetch_instance", value)


if not MYPY:
    class SparkJobDependentPackageArgsDict(TypedDict):
        group_name: pulumi.Input[_builtins.str]
        """
        Specifies the user group name.  
        Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
        Changing this parameter will submit a new spark job.
        """
        packages: pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgsDict']]]
        """
        Specifies the user group resource for details.
        Changing this parameter will submit a new spark job.
        The object structure is documented below.

        <a name="dependent_packages_packages"></a>
        The `packages` block supports:
        """
elif False:
    SparkJobDependentPackageArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class SparkJobDependentPackageArgs:
    def __init__(__self__, *,
                 group_name: pulumi.Input[_builtins.str],
                 packages: pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]):
        """
        :param pulumi.Input[_builtins.str] group_name: Specifies the user group name.  
               Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
               Changing this parameter will submit a new spark job.
        :param pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]] packages: Specifies the user group resource for details.
               Changing this parameter will submit a new spark job.
               The object structure is documented below.
               
               <a name="dependent_packages_packages"></a>
               The `packages` block supports:
        """
        pulumi.set(__self__, "group_name", group_name)
        pulumi.set(__self__, "packages", packages)

    @_builtins.property
    @pulumi.getter(name="groupName")
    def group_name(self) -> pulumi.Input[_builtins.str]:
        """
        Specifies the user group name.  
        Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "group_name")

    @group_name.setter
    def group_name(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "group_name", value)

    @_builtins.property
    @pulumi.getter
    def packages(self) -> pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]:
        """
        Specifies the user group resource for details.
        Changing this parameter will submit a new spark job.
        The object structure is documented below.

        <a name="dependent_packages_packages"></a>
        The `packages` block supports:
        """
        return pulumi.get(self, "packages")

    @packages.setter
    def packages(self, value: pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]):
        pulumi.set(self, "packages", value)


if not MYPY:
    class SparkJobDependentPackagePackageArgsDict(TypedDict):
        package_name: pulumi.Input[_builtins.str]
        """
        Specifies the resource name of the package.
        Changing this parameter will submit a new spark job.
        """
        type: pulumi.Input[_builtins.str]
        """
        Specifies the resource type of the package.
        Changing this parameter will submit a new spark job.
        """
elif False:
    SparkJobDependentPackagePackageArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class SparkJobDependentPackagePackageArgs:
    def __init__(__self__, *,
                 package_name: pulumi.Input[_builtins.str],
                 type: pulumi.Input[_builtins.str]):
        """
        :param pulumi.Input[_builtins.str] package_name: Specifies the resource name of the package.
               Changing this parameter will submit a new spark job.
        :param pulumi.Input[_builtins.str] type: Specifies the resource type of the package.
               Changing this parameter will submit a new spark job.
        """
        pulumi.set(__self__, "package_name", package_name)
        pulumi.set(__self__, "type", type)

    @_builtins.property
    @pulumi.getter(name="packageName")
    def package_name(self) -> pulumi.Input[_builtins.str]:
        """
        Specifies the resource name of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "package_name")

    @package_name.setter
    def package_name(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "package_name", value)

    @_builtins.property
    @pulumi.getter
    def type(self) -> pulumi.Input[_builtins.str]:
        """
        Specifies the resource type of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "type", value)


if not MYPY:
    class SparkTemplateBodyArgsDict(TypedDict):
        app_name: NotRequired[pulumi.Input[_builtins.str]]
        """
        Name of the package that is of the JAR or pyFile type.  
        You can also specify an OBS path, for example, obs://Bucket name/Package name.
        """
        app_parameters: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Input parameters of the main class, that is application parameters.
        """
        auto_recovery: NotRequired[pulumi.Input[_builtins.bool]]
        """
        Whether to enable the retry function.  
        If enabled, Spark jobs will be automatically retried after an exception occurs.
        The default value is false.
        """
        configurations: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]]]
        """
        The configuration items of the DLI spark.  
        For details, see [Spark configuration](https://spark.apache.org/docs/latest/configuration.html)
        If you want to enable the **access metadata** of DLI spark in HuaweiCloud, please set
        **spark.dli.metaAccess.enable** to **true**.
        """
        dependent_packages: NotRequired[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageArgsDict']]]]
        """
        The list of package resource objects.  
        The dependent_packages structure is documented below.
        """
        driver_cores: NotRequired[pulumi.Input[_builtins.int]]
        """
        Number of CPU cores of the Spark application driver.  
        This configuration item replaces the default parameter in **specification**.
        """
        driver_memory: NotRequired[pulumi.Input[_builtins.str]]
        """
        Driver memory of the Spark application, for example, 2 GB and 2048 MB.  
        This configuration item replaces the default parameter in **specification**.
        The unit must be provided. Otherwise, the startup fails.
        """
        executor_cores: NotRequired[pulumi.Input[_builtins.int]]
        """
        Number of CPU cores of each Executor in the Spark application.  
        This configuration item replaces the default parameter in **specification**.
        """
        executor_memory: NotRequired[pulumi.Input[_builtins.str]]
        """
        Executor memory of the Spark application, for example, 2 GB and 2048 MB.  
        This configuration item replaces the default parameter in **specification**.
        The unit must be provided. Otherwise, the startup fails.
        """
        files: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Name of the package that is of the file type and has been uploaded to the
        DLI resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        """
        jars: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Name of the package that is of the JAR type and has been uploaded to the DLI
        resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        """
        main_class: NotRequired[pulumi.Input[_builtins.str]]
        """
        Java/Spark main class of the template.
        """
        max_retry_times: NotRequired[pulumi.Input[_builtins.int]]
        """
        Maximum retry times.  
        The maximum value is 100, and the default value is 20.

        <a name="SparkTemplate_Resources"></a>
        The `resources` block supports:
        """
        modules: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Name of the dependent system resource module.
        DLI provides dependencies for executing datasource jobs.
        The dependent modules and corresponding services are as follows.
        + **sys.datasource.hbase**: CloudTable/MRS HBase
        + **sys.datasource.opentsdb**: CloudTable/MRS OpenTSDB
        + **sys.datasource.rds**: RDS MySQL
        + **sys.datasource.css**: CSS
        """
        name: NotRequired[pulumi.Input[_builtins.str]]
        """
        User group name.
        """
        num_executors: NotRequired[pulumi.Input[_builtins.int]]
        """
        Number of Executors in a Spark application.  
        This configuration item replaces the default parameter in **specification**.
        """
        obs_bucket: NotRequired[pulumi.Input[_builtins.str]]
        """
        OBS bucket for storing the Spark jobs.  
        Set this parameter when you need to save jobs.
        """
        python_files: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Name of the package that is of the PyFile type and has been uploaded to the DLI
        resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        """
        queue_name: NotRequired[pulumi.Input[_builtins.str]]
        """
        The DLI queue name.
        """
        resources: NotRequired[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyResourceArgsDict']]]]
        """
        User group resource.
        The resources structure is documented above.
        """
        specification: NotRequired[pulumi.Input[_builtins.str]]
        """
        Compute resource type. Currently, resource types A, B, and C are available.  
        The available types and related specifications are as follows, default to minimum configuration (type **A**).

        | type | resource | driver cores | executor cores | driver memory | executor memory | num executor |
        | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
        | A | 8 vCPUs, 32-GB memory | 2 | 1 | 7G | 4G | 6 |
        | B | 16 vCPUs, 64-GB memory | 2 | 2 | 7G | 8G | 7 |
        | C | 32 vCPUs, 128-GB memory | 4 | 2 | 12G | 8G | 14 |
        """
elif False:
    SparkTemplateBodyArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class SparkTemplateBodyArgs:
    def __init__(__self__, *,
                 app_name: Optional[pulumi.Input[_builtins.str]] = None,
                 app_parameters: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 auto_recovery: Optional[pulumi.Input[_builtins.bool]] = None,
                 configurations: Optional[pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]]] = None,
                 dependent_packages: Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageArgs']]]] = None,
                 driver_cores: Optional[pulumi.Input[_builtins.int]] = None,
                 driver_memory: Optional[pulumi.Input[_builtins.str]] = None,
                 executor_cores: Optional[pulumi.Input[_builtins.int]] = None,
                 executor_memory: Optional[pulumi.Input[_builtins.str]] = None,
                 files: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 jars: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 main_class: Optional[pulumi.Input[_builtins.str]] = None,
                 max_retry_times: Optional[pulumi.Input[_builtins.int]] = None,
                 modules: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 name: Optional[pulumi.Input[_builtins.str]] = None,
                 num_executors: Optional[pulumi.Input[_builtins.int]] = None,
                 obs_bucket: Optional[pulumi.Input[_builtins.str]] = None,
                 python_files: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 queue_name: Optional[pulumi.Input[_builtins.str]] = None,
                 resources: Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyResourceArgs']]]] = None,
                 specification: Optional[pulumi.Input[_builtins.str]] = None):
        """
        :param pulumi.Input[_builtins.str] app_name: Name of the package that is of the JAR or pyFile type.  
               You can also specify an OBS path, for example, obs://Bucket name/Package name.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] app_parameters: Input parameters of the main class, that is application parameters.
        :param pulumi.Input[_builtins.bool] auto_recovery: Whether to enable the retry function.  
               If enabled, Spark jobs will be automatically retried after an exception occurs.
               The default value is false.
        :param pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]] configurations: The configuration items of the DLI spark.  
               For details, see [Spark configuration](https://spark.apache.org/docs/latest/configuration.html)
               If you want to enable the **access metadata** of DLI spark in HuaweiCloud, please set
               **spark.dli.metaAccess.enable** to **true**.
        :param pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageArgs']]] dependent_packages: The list of package resource objects.  
               The dependent_packages structure is documented below.
        :param pulumi.Input[_builtins.int] driver_cores: Number of CPU cores of the Spark application driver.  
               This configuration item replaces the default parameter in **specification**.
        :param pulumi.Input[_builtins.str] driver_memory: Driver memory of the Spark application, for example, 2 GB and 2048 MB.  
               This configuration item replaces the default parameter in **specification**.
               The unit must be provided. Otherwise, the startup fails.
        :param pulumi.Input[_builtins.int] executor_cores: Number of CPU cores of each Executor in the Spark application.  
               This configuration item replaces the default parameter in **specification**.
        :param pulumi.Input[_builtins.str] executor_memory: Executor memory of the Spark application, for example, 2 GB and 2048 MB.  
               This configuration item replaces the default parameter in **specification**.
               The unit must be provided. Otherwise, the startup fails.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] files: Name of the package that is of the file type and has been uploaded to the
               DLI resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] jars: Name of the package that is of the JAR type and has been uploaded to the DLI
               resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        :param pulumi.Input[_builtins.str] main_class: Java/Spark main class of the template.
        :param pulumi.Input[_builtins.int] max_retry_times: Maximum retry times.  
               The maximum value is 100, and the default value is 20.
               
               <a name="SparkTemplate_Resources"></a>
               The `resources` block supports:
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] modules: Name of the dependent system resource module.
               DLI provides dependencies for executing datasource jobs.
               The dependent modules and corresponding services are as follows.
               + **sys.datasource.hbase**: CloudTable/MRS HBase
               + **sys.datasource.opentsdb**: CloudTable/MRS OpenTSDB
               + **sys.datasource.rds**: RDS MySQL
               + **sys.datasource.css**: CSS
        :param pulumi.Input[_builtins.str] name: User group name.
        :param pulumi.Input[_builtins.int] num_executors: Number of Executors in a Spark application.  
               This configuration item replaces the default parameter in **specification**.
        :param pulumi.Input[_builtins.str] obs_bucket: OBS bucket for storing the Spark jobs.  
               Set this parameter when you need to save jobs.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] python_files: Name of the package that is of the PyFile type and has been uploaded to the DLI
               resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        :param pulumi.Input[_builtins.str] queue_name: The DLI queue name.
        :param pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyResourceArgs']]] resources: User group resource.
               The resources structure is documented above.
        :param pulumi.Input[_builtins.str] specification: Compute resource type. Currently, resource types A, B, and C are available.  
               The available types and related specifications are as follows, default to minimum configuration (type **A**).
               
               | type | resource | driver cores | executor cores | driver memory | executor memory | num executor |
               | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
               | A | 8 vCPUs, 32-GB memory | 2 | 1 | 7G | 4G | 6 |
               | B | 16 vCPUs, 64-GB memory | 2 | 2 | 7G | 8G | 7 |
               | C | 32 vCPUs, 128-GB memory | 4 | 2 | 12G | 8G | 14 |
        """
        if app_name is not None:
            pulumi.set(__self__, "app_name", app_name)
        if app_parameters is not None:
            pulumi.set(__self__, "app_parameters", app_parameters)
        if auto_recovery is not None:
            pulumi.set(__self__, "auto_recovery", auto_recovery)
        if configurations is not None:
            pulumi.set(__self__, "configurations", configurations)
        if dependent_packages is not None:
            pulumi.set(__self__, "dependent_packages", dependent_packages)
        if driver_cores is not None:
            pulumi.set(__self__, "driver_cores", driver_cores)
        if driver_memory is not None:
            pulumi.set(__self__, "driver_memory", driver_memory)
        if executor_cores is not None:
            pulumi.set(__self__, "executor_cores", executor_cores)
        if executor_memory is not None:
            pulumi.set(__self__, "executor_memory", executor_memory)
        if files is not None:
            pulumi.set(__self__, "files", files)
        if jars is not None:
            pulumi.set(__self__, "jars", jars)
        if main_class is not None:
            pulumi.set(__self__, "main_class", main_class)
        if max_retry_times is not None:
            pulumi.set(__self__, "max_retry_times", max_retry_times)
        if modules is not None:
            pulumi.set(__self__, "modules", modules)
        if name is not None:
            pulumi.set(__self__, "name", name)
        if num_executors is not None:
            pulumi.set(__self__, "num_executors", num_executors)
        if obs_bucket is not None:
            pulumi.set(__self__, "obs_bucket", obs_bucket)
        if python_files is not None:
            pulumi.set(__self__, "python_files", python_files)
        if queue_name is not None:
            pulumi.set(__self__, "queue_name", queue_name)
        if resources is not None:
            pulumi.set(__self__, "resources", resources)
        if specification is not None:
            pulumi.set(__self__, "specification", specification)

    @_builtins.property
    @pulumi.getter(name="appName")
    def app_name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Name of the package that is of the JAR or pyFile type.  
        You can also specify an OBS path, for example, obs://Bucket name/Package name.
        """
        return pulumi.get(self, "app_name")

    @app_name.setter
    def app_name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "app_name", value)

    @_builtins.property
    @pulumi.getter(name="appParameters")
    def app_parameters(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Input parameters of the main class, that is application parameters.
        """
        return pulumi.get(self, "app_parameters")

    @app_parameters.setter
    def app_parameters(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "app_parameters", value)

    @_builtins.property
    @pulumi.getter(name="autoRecovery")
    def auto_recovery(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        Whether to enable the retry function.  
        If enabled, Spark jobs will be automatically retried after an exception occurs.
        The default value is false.
        """
        return pulumi.get(self, "auto_recovery")

    @auto_recovery.setter
    def auto_recovery(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "auto_recovery", value)

    @_builtins.property
    @pulumi.getter
    def configurations(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]]]:
        """
        The configuration items of the DLI spark.  
        For details, see [Spark configuration](https://spark.apache.org/docs/latest/configuration.html)
        If you want to enable the **access metadata** of DLI spark in HuaweiCloud, please set
        **spark.dli.metaAccess.enable** to **true**.
        """
        return pulumi.get(self, "configurations")

    @configurations.setter
    def configurations(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "configurations", value)

    @_builtins.property
    @pulumi.getter(name="dependentPackages")
    def dependent_packages(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageArgs']]]]:
        """
        The list of package resource objects.  
        The dependent_packages structure is documented below.
        """
        return pulumi.get(self, "dependent_packages")

    @dependent_packages.setter
    def dependent_packages(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageArgs']]]]):
        pulumi.set(self, "dependent_packages", value)

    @_builtins.property
    @pulumi.getter(name="driverCores")
    def driver_cores(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Number of CPU cores of the Spark application driver.  
        This configuration item replaces the default parameter in **specification**.
        """
        return pulumi.get(self, "driver_cores")

    @driver_cores.setter
    def driver_cores(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "driver_cores", value)

    @_builtins.property
    @pulumi.getter(name="driverMemory")
    def driver_memory(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Driver memory of the Spark application, for example, 2 GB and 2048 MB.  
        This configuration item replaces the default parameter in **specification**.
        The unit must be provided. Otherwise, the startup fails.
        """
        return pulumi.get(self, "driver_memory")

    @driver_memory.setter
    def driver_memory(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "driver_memory", value)

    @_builtins.property
    @pulumi.getter(name="executorCores")
    def executor_cores(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Number of CPU cores of each Executor in the Spark application.  
        This configuration item replaces the default parameter in **specification**.
        """
        return pulumi.get(self, "executor_cores")

    @executor_cores.setter
    def executor_cores(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "executor_cores", value)

    @_builtins.property
    @pulumi.getter(name="executorMemory")
    def executor_memory(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Executor memory of the Spark application, for example, 2 GB and 2048 MB.  
        This configuration item replaces the default parameter in **specification**.
        The unit must be provided. Otherwise, the startup fails.
        """
        return pulumi.get(self, "executor_memory")

    @executor_memory.setter
    def executor_memory(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "executor_memory", value)

    @_builtins.property
    @pulumi.getter
    def files(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Name of the package that is of the file type and has been uploaded to the
        DLI resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        """
        return pulumi.get(self, "files")

    @files.setter
    def files(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "files", value)

    @_builtins.property
    @pulumi.getter
    def jars(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Name of the package that is of the JAR type and has been uploaded to the DLI
        resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        """
        return pulumi.get(self, "jars")

    @jars.setter
    def jars(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "jars", value)

    @_builtins.property
    @pulumi.getter(name="mainClass")
    def main_class(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Java/Spark main class of the template.
        """
        return pulumi.get(self, "main_class")

    @main_class.setter
    def main_class(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "main_class", value)

    @_builtins.property
    @pulumi.getter(name="maxRetryTimes")
    def max_retry_times(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Maximum retry times.  
        The maximum value is 100, and the default value is 20.

        <a name="SparkTemplate_Resources"></a>
        The `resources` block supports:
        """
        return pulumi.get(self, "max_retry_times")

    @max_retry_times.setter
    def max_retry_times(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "max_retry_times", value)

    @_builtins.property
    @pulumi.getter
    def modules(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Name of the dependent system resource module.
        DLI provides dependencies for executing datasource jobs.
        The dependent modules and corresponding services are as follows.
        + **sys.datasource.hbase**: CloudTable/MRS HBase
        + **sys.datasource.opentsdb**: CloudTable/MRS OpenTSDB
        + **sys.datasource.rds**: RDS MySQL
        + **sys.datasource.css**: CSS
        """
        return pulumi.get(self, "modules")

    @modules.setter
    def modules(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "modules", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        User group name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter(name="numExecutors")
    def num_executors(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Number of Executors in a Spark application.  
        This configuration item replaces the default parameter in **specification**.
        """
        return pulumi.get(self, "num_executors")

    @num_executors.setter
    def num_executors(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "num_executors", value)

    @_builtins.property
    @pulumi.getter(name="obsBucket")
    def obs_bucket(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        OBS bucket for storing the Spark jobs.  
        Set this parameter when you need to save jobs.
        """
        return pulumi.get(self, "obs_bucket")

    @obs_bucket.setter
    def obs_bucket(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "obs_bucket", value)

    @_builtins.property
    @pulumi.getter(name="pythonFiles")
    def python_files(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Name of the package that is of the PyFile type and has been uploaded to the DLI
        resource management system. You can also specify an OBS path, for example, obs://Bucket name/Package name.
        """
        return pulumi.get(self, "python_files")

    @python_files.setter
    def python_files(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "python_files", value)

    @_builtins.property
    @pulumi.getter(name="queueName")
    def queue_name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        The DLI queue name.
        """
        return pulumi.get(self, "queue_name")

    @queue_name.setter
    def queue_name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "queue_name", value)

    @_builtins.property
    @pulumi.getter
    def resources(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyResourceArgs']]]]:
        """
        User group resource.
        The resources structure is documented above.
        """
        return pulumi.get(self, "resources")

    @resources.setter
    def resources(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyResourceArgs']]]]):
        pulumi.set(self, "resources", value)

    @_builtins.property
    @pulumi.getter
    def specification(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Compute resource type. Currently, resource types A, B, and C are available.  
        The available types and related specifications are as follows, default to minimum configuration (type **A**).

        | type | resource | driver cores | executor cores | driver memory | executor memory | num executor |
        | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
        | A | 8 vCPUs, 32-GB memory | 2 | 1 | 7G | 4G | 6 |
        | B | 16 vCPUs, 64-GB memory | 2 | 2 | 7G | 8G | 7 |
        | C | 32 vCPUs, 128-GB memory | 4 | 2 | 12G | 8G | 14 |
        """
        return pulumi.get(self, "specification")

    @specification.setter
    def specification(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "specification", value)


if not MYPY:
    class SparkTemplateBodyDependentPackageArgsDict(TypedDict):
        name: NotRequired[pulumi.Input[_builtins.str]]
        """
        User group name.
        """
        resources: NotRequired[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageResourceArgsDict']]]]
        """
        User group resource.
        The resources structure is documented above.
        """
elif False:
    SparkTemplateBodyDependentPackageArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class SparkTemplateBodyDependentPackageArgs:
    def __init__(__self__, *,
                 name: Optional[pulumi.Input[_builtins.str]] = None,
                 resources: Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageResourceArgs']]]] = None):
        """
        :param pulumi.Input[_builtins.str] name: User group name.
        :param pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageResourceArgs']]] resources: User group resource.
               The resources structure is documented above.
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if resources is not None:
            pulumi.set(__self__, "resources", resources)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        User group name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter
    def resources(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageResourceArgs']]]]:
        """
        User group resource.
        The resources structure is documented above.
        """
        return pulumi.get(self, "resources")

    @resources.setter
    def resources(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['SparkTemplateBodyDependentPackageResourceArgs']]]]):
        pulumi.set(self, "resources", value)


if not MYPY:
    class SparkTemplateBodyDependentPackageResourceArgsDict(TypedDict):
        name: NotRequired[pulumi.Input[_builtins.str]]
        """
        User group name.
        """
        type: NotRequired[pulumi.Input[_builtins.str]]
        """
        Resource type.

        <a name="SparkTemplate_Dependent_packages"></a>
        The `dependent_packages` block supports:
        """
elif False:
    SparkTemplateBodyDependentPackageResourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class SparkTemplateBodyDependentPackageResourceArgs:
    def __init__(__self__, *,
                 name: Optional[pulumi.Input[_builtins.str]] = None,
                 type: Optional[pulumi.Input[_builtins.str]] = None):
        """
        :param pulumi.Input[_builtins.str] name: User group name.
        :param pulumi.Input[_builtins.str] type: Resource type.
               
               <a name="SparkTemplate_Dependent_packages"></a>
               The `dependent_packages` block supports:
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if type is not None:
            pulumi.set(__self__, "type", type)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        User group name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter
    def type(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Resource type.

        <a name="SparkTemplate_Dependent_packages"></a>
        The `dependent_packages` block supports:
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "type", value)


if not MYPY:
    class SparkTemplateBodyResourceArgsDict(TypedDict):
        name: NotRequired[pulumi.Input[_builtins.str]]
        """
        User group name.
        """
        type: NotRequired[pulumi.Input[_builtins.str]]
        """
        Resource type.

        <a name="SparkTemplate_Dependent_packages"></a>
        The `dependent_packages` block supports:
        """
elif False:
    SparkTemplateBodyResourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class SparkTemplateBodyResourceArgs:
    def __init__(__self__, *,
                 name: Optional[pulumi.Input[_builtins.str]] = None,
                 type: Optional[pulumi.Input[_builtins.str]] = None):
        """
        :param pulumi.Input[_builtins.str] name: User group name.
        :param pulumi.Input[_builtins.str] type: Resource type.
               
               <a name="SparkTemplate_Dependent_packages"></a>
               The `dependent_packages` block supports:
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if type is not None:
            pulumi.set(__self__, "type", type)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        User group name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter
    def type(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Resource type.

        <a name="SparkTemplate_Dependent_packages"></a>
        The `dependent_packages` block supports:
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "type", value)


if not MYPY:
    class SqlJobConfArgsDict(TypedDict):
        dli_sql_job_timeout: NotRequired[pulumi.Input[_builtins.int]]
        """
        Sets the job running timeout interval. If the timeout interval
        expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        """
        dli_sql_sqlasync_enabled: NotRequired[pulumi.Input[_builtins.bool]]
        """
        Specifies whether DDL and DCL statements are executed
        asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        spark_sql_auto_broadcast_join_threshold: NotRequired[pulumi.Input[_builtins.int]]
        """
        Maximum size of the table that
        displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
        Default value is `209715200`. Changing this parameter will create a new resource.

        > Currently, only the configuration unit metastore table that runs the ANALYZE TABLE COMPUTE statistics noscan
        command and the file-based data source table that directly calculates statistics based on data files are supported.
        Changing this parameter will create a new resource.
        """
        spark_sql_bad_records_path: NotRequired[pulumi.Input[_builtins.str]]
        """
        Path of bad records. Changing this parameter will create
        a new resource.
        """
        spark_sql_dynamic_partition_overwrite_enabled: NotRequired[pulumi.Input[_builtins.bool]]
        """
        In dynamic mode, Spark does not delete
        the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        spark_sql_files_max_partition_bytes: NotRequired[pulumi.Input[_builtins.int]]
        """
        Maximum number of bytes to be packed into a
        single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
        resource.
        """
        spark_sql_max_records_per_file: NotRequired[pulumi.Input[_builtins.int]]
        """
        Maximum number of records to be written
        into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
        Changing this parameter will create a new resource.
        """
        spark_sql_shuffle_partitions: NotRequired[pulumi.Input[_builtins.int]]
        """
        Default number of partitions used to filter
        data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
elif False:
    SqlJobConfArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class SqlJobConfArgs:
    def __init__(__self__, *,
                 dli_sql_job_timeout: Optional[pulumi.Input[_builtins.int]] = None,
                 dli_sql_sqlasync_enabled: Optional[pulumi.Input[_builtins.bool]] = None,
                 spark_sql_auto_broadcast_join_threshold: Optional[pulumi.Input[_builtins.int]] = None,
                 spark_sql_bad_records_path: Optional[pulumi.Input[_builtins.str]] = None,
                 spark_sql_dynamic_partition_overwrite_enabled: Optional[pulumi.Input[_builtins.bool]] = None,
                 spark_sql_files_max_partition_bytes: Optional[pulumi.Input[_builtins.int]] = None,
                 spark_sql_max_records_per_file: Optional[pulumi.Input[_builtins.int]] = None,
                 spark_sql_shuffle_partitions: Optional[pulumi.Input[_builtins.int]] = None):
        """
        :param pulumi.Input[_builtins.int] dli_sql_job_timeout: Sets the job running timeout interval. If the timeout interval
               expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        :param pulumi.Input[_builtins.bool] dli_sql_sqlasync_enabled: Specifies whether DDL and DCL statements are executed
               asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[_builtins.int] spark_sql_auto_broadcast_join_threshold: Maximum size of the table that
               displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
               Default value is `209715200`. Changing this parameter will create a new resource.
               
               > Currently, only the configuration unit metastore table that runs the ANALYZE TABLE COMPUTE statistics noscan
               command and the file-based data source table that directly calculates statistics based on data files are supported.
               Changing this parameter will create a new resource.
        :param pulumi.Input[_builtins.str] spark_sql_bad_records_path: Path of bad records. Changing this parameter will create
               a new resource.
        :param pulumi.Input[_builtins.bool] spark_sql_dynamic_partition_overwrite_enabled: In dynamic mode, Spark does not delete
               the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[_builtins.int] spark_sql_files_max_partition_bytes: Maximum number of bytes to be packed into a
               single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
               resource.
        :param pulumi.Input[_builtins.int] spark_sql_max_records_per_file: Maximum number of records to be written
               into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[_builtins.int] spark_sql_shuffle_partitions: Default number of partitions used to filter
               data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        if dli_sql_job_timeout is not None:
            pulumi.set(__self__, "dli_sql_job_timeout", dli_sql_job_timeout)
        if dli_sql_sqlasync_enabled is not None:
            pulumi.set(__self__, "dli_sql_sqlasync_enabled", dli_sql_sqlasync_enabled)
        if spark_sql_auto_broadcast_join_threshold is not None:
            pulumi.set(__self__, "spark_sql_auto_broadcast_join_threshold", spark_sql_auto_broadcast_join_threshold)
        if spark_sql_bad_records_path is not None:
            pulumi.set(__self__, "spark_sql_bad_records_path", spark_sql_bad_records_path)
        if spark_sql_dynamic_partition_overwrite_enabled is not None:
            pulumi.set(__self__, "spark_sql_dynamic_partition_overwrite_enabled", spark_sql_dynamic_partition_overwrite_enabled)
        if spark_sql_files_max_partition_bytes is not None:
            pulumi.set(__self__, "spark_sql_files_max_partition_bytes", spark_sql_files_max_partition_bytes)
        if spark_sql_max_records_per_file is not None:
            pulumi.set(__self__, "spark_sql_max_records_per_file", spark_sql_max_records_per_file)
        if spark_sql_shuffle_partitions is not None:
            pulumi.set(__self__, "spark_sql_shuffle_partitions", spark_sql_shuffle_partitions)

    @_builtins.property
    @pulumi.getter(name="dliSqlJobTimeout")
    def dli_sql_job_timeout(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Sets the job running timeout interval. If the timeout interval
        expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_job_timeout")

    @dli_sql_job_timeout.setter
    def dli_sql_job_timeout(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "dli_sql_job_timeout", value)

    @_builtins.property
    @pulumi.getter(name="dliSqlSqlasyncEnabled")
    def dli_sql_sqlasync_enabled(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        Specifies whether DDL and DCL statements are executed
        asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_sqlasync_enabled")

    @dli_sql_sqlasync_enabled.setter
    def dli_sql_sqlasync_enabled(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "dli_sql_sqlasync_enabled", value)

    @_builtins.property
    @pulumi.getter(name="sparkSqlAutoBroadcastJoinThreshold")
    def spark_sql_auto_broadcast_join_threshold(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Maximum size of the table that
        displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
        Default value is `209715200`. Changing this parameter will create a new resource.

        > Currently, only the configuration unit metastore table that runs the ANALYZE TABLE COMPUTE statistics noscan
        command and the file-based data source table that directly calculates statistics based on data files are supported.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_auto_broadcast_join_threshold")

    @spark_sql_auto_broadcast_join_threshold.setter
    def spark_sql_auto_broadcast_join_threshold(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "spark_sql_auto_broadcast_join_threshold", value)

    @_builtins.property
    @pulumi.getter(name="sparkSqlBadRecordsPath")
    def spark_sql_bad_records_path(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Path of bad records. Changing this parameter will create
        a new resource.
        """
        return pulumi.get(self, "spark_sql_bad_records_path")

    @spark_sql_bad_records_path.setter
    def spark_sql_bad_records_path(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "spark_sql_bad_records_path", value)

    @_builtins.property
    @pulumi.getter(name="sparkSqlDynamicPartitionOverwriteEnabled")
    def spark_sql_dynamic_partition_overwrite_enabled(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        In dynamic mode, Spark does not delete
        the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_dynamic_partition_overwrite_enabled")

    @spark_sql_dynamic_partition_overwrite_enabled.setter
    def spark_sql_dynamic_partition_overwrite_enabled(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "spark_sql_dynamic_partition_overwrite_enabled", value)

    @_builtins.property
    @pulumi.getter(name="sparkSqlFilesMaxPartitionBytes")
    def spark_sql_files_max_partition_bytes(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Maximum number of bytes to be packed into a
        single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "spark_sql_files_max_partition_bytes")

    @spark_sql_files_max_partition_bytes.setter
    def spark_sql_files_max_partition_bytes(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "spark_sql_files_max_partition_bytes", value)

    @_builtins.property
    @pulumi.getter(name="sparkSqlMaxRecordsPerFile")
    def spark_sql_max_records_per_file(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Maximum number of records to be written
        into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_max_records_per_file")

    @spark_sql_max_records_per_file.setter
    def spark_sql_max_records_per_file(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "spark_sql_max_records_per_file", value)

    @_builtins.property
    @pulumi.getter(name="sparkSqlShufflePartitions")
    def spark_sql_shuffle_partitions(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Default number of partitions used to filter
        data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_shuffle_partitions")

    @spark_sql_shuffle_partitions.setter
    def spark_sql_shuffle_partitions(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "spark_sql_shuffle_partitions", value)


if not MYPY:
    class TableColumnArgsDict(TypedDict):
        name: pulumi.Input[_builtins.str]
        """
        Specifies the table name. The name can contain only digits, letters,
        and underscores, but cannot contain only digits or start with an underscore. Length range: 1 to 128 characters.
        Changing this parameter will create a new resource.
        """
        type: pulumi.Input[_builtins.str]
        """
        Specifies data type of column. Changing this parameter will create a new
        resource.
        """
        description: NotRequired[pulumi.Input[_builtins.str]]
        """
        Specifies description of the table.
        Changing this parameter will create a new resource.
        """
        is_partition: NotRequired[pulumi.Input[_builtins.bool]]
        """
        Specifies whether the column is a partition column. The value
        `true` indicates a partition column, and the value false indicates a non-partition column. The default value
        is false. Changing this parameter will create a new resource.

        > When creating a partition table, ensure that at least one column in the table is a non-partition column.
        """
elif False:
    TableColumnArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class TableColumnArgs:
    def __init__(__self__, *,
                 name: pulumi.Input[_builtins.str],
                 type: pulumi.Input[_builtins.str],
                 description: Optional[pulumi.Input[_builtins.str]] = None,
                 is_partition: Optional[pulumi.Input[_builtins.bool]] = None):
        """
        :param pulumi.Input[_builtins.str] name: Specifies the table name. The name can contain only digits, letters,
               and underscores, but cannot contain only digits or start with an underscore. Length range: 1 to 128 characters.
               Changing this parameter will create a new resource.
        :param pulumi.Input[_builtins.str] type: Specifies data type of column. Changing this parameter will create a new
               resource.
        :param pulumi.Input[_builtins.str] description: Specifies description of the table.
               Changing this parameter will create a new resource.
        :param pulumi.Input[_builtins.bool] is_partition: Specifies whether the column is a partition column. The value
               `true` indicates a partition column, and the value false indicates a non-partition column. The default value
               is false. Changing this parameter will create a new resource.
               
               > When creating a partition table, ensure that at least one column in the table is a non-partition column.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "type", type)
        if description is not None:
            pulumi.set(__self__, "description", description)
        if is_partition is not None:
            pulumi.set(__self__, "is_partition", is_partition)

    @_builtins.property
    @pulumi.getter
    def name(self) -> pulumi.Input[_builtins.str]:
        """
        Specifies the table name. The name can contain only digits, letters,
        and underscores, but cannot contain only digits or start with an underscore. Length range: 1 to 128 characters.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter
    def type(self) -> pulumi.Input[_builtins.str]:
        """
        Specifies data type of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "type", value)

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Specifies description of the table.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "description", value)

    @_builtins.property
    @pulumi.getter(name="isPartition")
    def is_partition(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        Specifies whether the column is a partition column. The value
        `true` indicates a partition column, and the value false indicates a non-partition column. The default value
        is false. Changing this parameter will create a new resource.

        > When creating a partition table, ensure that at least one column in the table is a non-partition column.
        """
        return pulumi.get(self, "is_partition")

    @is_partition.setter
    def is_partition(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "is_partition", value)


if not MYPY:
    class TemplateSparkBodyArgsDict(TypedDict):
        app_name: NotRequired[pulumi.Input[_builtins.str]]
        """
        Name of the package that is of the JAR or pyFile type.
        """
        app_parameters: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Input parameters of the main class, that is application parameters.
        """
        auto_recovery: NotRequired[pulumi.Input[_builtins.bool]]
        """
        Whether to enable the retry function.
        """
        configurations: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]]]
        """
        The configuration items of the DLI spark.
        """
        dependent_packages: NotRequired[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageArgsDict']]]]
        """
        The list of package resource objects.
        """
        driver_cores: NotRequired[pulumi.Input[_builtins.int]]
        """
        Number of CPU cores of the Spark application driver.
        """
        driver_memory: NotRequired[pulumi.Input[_builtins.str]]
        """
        Driver memory of the Spark application, for example, 2 GB and 2048 MB.
        """
        executor_cores: NotRequired[pulumi.Input[_builtins.int]]
        """
        Number of CPU cores of each Executor in the Spark application.
        """
        executor_memory: NotRequired[pulumi.Input[_builtins.str]]
        """
        Executor memory of the Spark application, for example, 2 GB and 2048 MB.
        """
        files: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Name of the package that is of the file type and has been uploaded to the DLI resource management system.
        """
        jars: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Name of the package that is of the JAR type and has been uploaded to the DLI resource management system.
        """
        main_class: NotRequired[pulumi.Input[_builtins.str]]
        """
        Java/Spark main class of the template.
        """
        max_retry_times: NotRequired[pulumi.Input[_builtins.int]]
        """
        Maximum retry times.
        """
        modules: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Name of the dependent system resource module.
        """
        name: NotRequired[pulumi.Input[_builtins.str]]
        """
        The spark job name.
        """
        num_executors: NotRequired[pulumi.Input[_builtins.int]]
        """
        Number of Executors in a Spark application.
        """
        obs_bucket: NotRequired[pulumi.Input[_builtins.str]]
        """
        OBS bucket for storing the Spark jobs.
        """
        python_files: NotRequired[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]
        """
        Name of the package that is of the PyFile type and has been uploaded to the DLI resource management system.
        """
        queue_name: NotRequired[pulumi.Input[_builtins.str]]
        """
        The DLI queue name.
        """
        resources: NotRequired[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyResourceArgsDict']]]]
        """
        The list of resource objects.
        """
        specification: NotRequired[pulumi.Input[_builtins.str]]
        """
        Compute resource type. Currently, resource types A, B, and C are available.
        """
elif False:
    TemplateSparkBodyArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class TemplateSparkBodyArgs:
    def __init__(__self__, *,
                 app_name: Optional[pulumi.Input[_builtins.str]] = None,
                 app_parameters: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 auto_recovery: Optional[pulumi.Input[_builtins.bool]] = None,
                 configurations: Optional[pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]]] = None,
                 dependent_packages: Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageArgs']]]] = None,
                 driver_cores: Optional[pulumi.Input[_builtins.int]] = None,
                 driver_memory: Optional[pulumi.Input[_builtins.str]] = None,
                 executor_cores: Optional[pulumi.Input[_builtins.int]] = None,
                 executor_memory: Optional[pulumi.Input[_builtins.str]] = None,
                 files: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 jars: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 main_class: Optional[pulumi.Input[_builtins.str]] = None,
                 max_retry_times: Optional[pulumi.Input[_builtins.int]] = None,
                 modules: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 name: Optional[pulumi.Input[_builtins.str]] = None,
                 num_executors: Optional[pulumi.Input[_builtins.int]] = None,
                 obs_bucket: Optional[pulumi.Input[_builtins.str]] = None,
                 python_files: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]] = None,
                 queue_name: Optional[pulumi.Input[_builtins.str]] = None,
                 resources: Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyResourceArgs']]]] = None,
                 specification: Optional[pulumi.Input[_builtins.str]] = None):
        """
        :param pulumi.Input[_builtins.str] app_name: Name of the package that is of the JAR or pyFile type.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] app_parameters: Input parameters of the main class, that is application parameters.
        :param pulumi.Input[_builtins.bool] auto_recovery: Whether to enable the retry function.
        :param pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]] configurations: The configuration items of the DLI spark.
        :param pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageArgs']]] dependent_packages: The list of package resource objects.
        :param pulumi.Input[_builtins.int] driver_cores: Number of CPU cores of the Spark application driver.
        :param pulumi.Input[_builtins.str] driver_memory: Driver memory of the Spark application, for example, 2 GB and 2048 MB.
        :param pulumi.Input[_builtins.int] executor_cores: Number of CPU cores of each Executor in the Spark application.
        :param pulumi.Input[_builtins.str] executor_memory: Executor memory of the Spark application, for example, 2 GB and 2048 MB.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] files: Name of the package that is of the file type and has been uploaded to the DLI resource management system.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] jars: Name of the package that is of the JAR type and has been uploaded to the DLI resource management system.
        :param pulumi.Input[_builtins.str] main_class: Java/Spark main class of the template.
        :param pulumi.Input[_builtins.int] max_retry_times: Maximum retry times.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] modules: Name of the dependent system resource module.
        :param pulumi.Input[_builtins.str] name: The spark job name.
        :param pulumi.Input[_builtins.int] num_executors: Number of Executors in a Spark application.
        :param pulumi.Input[_builtins.str] obs_bucket: OBS bucket for storing the Spark jobs.
        :param pulumi.Input[Sequence[pulumi.Input[_builtins.str]]] python_files: Name of the package that is of the PyFile type and has been uploaded to the DLI resource management system.
        :param pulumi.Input[_builtins.str] queue_name: The DLI queue name.
        :param pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyResourceArgs']]] resources: The list of resource objects.
        :param pulumi.Input[_builtins.str] specification: Compute resource type. Currently, resource types A, B, and C are available.
        """
        if app_name is not None:
            pulumi.set(__self__, "app_name", app_name)
        if app_parameters is not None:
            pulumi.set(__self__, "app_parameters", app_parameters)
        if auto_recovery is not None:
            pulumi.set(__self__, "auto_recovery", auto_recovery)
        if configurations is not None:
            pulumi.set(__self__, "configurations", configurations)
        if dependent_packages is not None:
            pulumi.set(__self__, "dependent_packages", dependent_packages)
        if driver_cores is not None:
            pulumi.set(__self__, "driver_cores", driver_cores)
        if driver_memory is not None:
            pulumi.set(__self__, "driver_memory", driver_memory)
        if executor_cores is not None:
            pulumi.set(__self__, "executor_cores", executor_cores)
        if executor_memory is not None:
            pulumi.set(__self__, "executor_memory", executor_memory)
        if files is not None:
            pulumi.set(__self__, "files", files)
        if jars is not None:
            pulumi.set(__self__, "jars", jars)
        if main_class is not None:
            pulumi.set(__self__, "main_class", main_class)
        if max_retry_times is not None:
            pulumi.set(__self__, "max_retry_times", max_retry_times)
        if modules is not None:
            pulumi.set(__self__, "modules", modules)
        if name is not None:
            pulumi.set(__self__, "name", name)
        if num_executors is not None:
            pulumi.set(__self__, "num_executors", num_executors)
        if obs_bucket is not None:
            pulumi.set(__self__, "obs_bucket", obs_bucket)
        if python_files is not None:
            pulumi.set(__self__, "python_files", python_files)
        if queue_name is not None:
            pulumi.set(__self__, "queue_name", queue_name)
        if resources is not None:
            pulumi.set(__self__, "resources", resources)
        if specification is not None:
            pulumi.set(__self__, "specification", specification)

    @_builtins.property
    @pulumi.getter(name="appName")
    def app_name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Name of the package that is of the JAR or pyFile type.
        """
        return pulumi.get(self, "app_name")

    @app_name.setter
    def app_name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "app_name", value)

    @_builtins.property
    @pulumi.getter(name="appParameters")
    def app_parameters(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Input parameters of the main class, that is application parameters.
        """
        return pulumi.get(self, "app_parameters")

    @app_parameters.setter
    def app_parameters(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "app_parameters", value)

    @_builtins.property
    @pulumi.getter(name="autoRecovery")
    def auto_recovery(self) -> Optional[pulumi.Input[_builtins.bool]]:
        """
        Whether to enable the retry function.
        """
        return pulumi.get(self, "auto_recovery")

    @auto_recovery.setter
    def auto_recovery(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "auto_recovery", value)

    @_builtins.property
    @pulumi.getter
    def configurations(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]]]:
        """
        The configuration items of the DLI spark.
        """
        return pulumi.get(self, "configurations")

    @configurations.setter
    def configurations(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "configurations", value)

    @_builtins.property
    @pulumi.getter(name="dependentPackages")
    def dependent_packages(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageArgs']]]]:
        """
        The list of package resource objects.
        """
        return pulumi.get(self, "dependent_packages")

    @dependent_packages.setter
    def dependent_packages(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageArgs']]]]):
        pulumi.set(self, "dependent_packages", value)

    @_builtins.property
    @pulumi.getter(name="driverCores")
    def driver_cores(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Number of CPU cores of the Spark application driver.
        """
        return pulumi.get(self, "driver_cores")

    @driver_cores.setter
    def driver_cores(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "driver_cores", value)

    @_builtins.property
    @pulumi.getter(name="driverMemory")
    def driver_memory(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Driver memory of the Spark application, for example, 2 GB and 2048 MB.
        """
        return pulumi.get(self, "driver_memory")

    @driver_memory.setter
    def driver_memory(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "driver_memory", value)

    @_builtins.property
    @pulumi.getter(name="executorCores")
    def executor_cores(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Number of CPU cores of each Executor in the Spark application.
        """
        return pulumi.get(self, "executor_cores")

    @executor_cores.setter
    def executor_cores(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "executor_cores", value)

    @_builtins.property
    @pulumi.getter(name="executorMemory")
    def executor_memory(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Executor memory of the Spark application, for example, 2 GB and 2048 MB.
        """
        return pulumi.get(self, "executor_memory")

    @executor_memory.setter
    def executor_memory(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "executor_memory", value)

    @_builtins.property
    @pulumi.getter
    def files(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Name of the package that is of the file type and has been uploaded to the DLI resource management system.
        """
        return pulumi.get(self, "files")

    @files.setter
    def files(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "files", value)

    @_builtins.property
    @pulumi.getter
    def jars(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Name of the package that is of the JAR type and has been uploaded to the DLI resource management system.
        """
        return pulumi.get(self, "jars")

    @jars.setter
    def jars(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "jars", value)

    @_builtins.property
    @pulumi.getter(name="mainClass")
    def main_class(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Java/Spark main class of the template.
        """
        return pulumi.get(self, "main_class")

    @main_class.setter
    def main_class(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "main_class", value)

    @_builtins.property
    @pulumi.getter(name="maxRetryTimes")
    def max_retry_times(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Maximum retry times.
        """
        return pulumi.get(self, "max_retry_times")

    @max_retry_times.setter
    def max_retry_times(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "max_retry_times", value)

    @_builtins.property
    @pulumi.getter
    def modules(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Name of the dependent system resource module.
        """
        return pulumi.get(self, "modules")

    @modules.setter
    def modules(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "modules", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        The spark job name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter(name="numExecutors")
    def num_executors(self) -> Optional[pulumi.Input[_builtins.int]]:
        """
        Number of Executors in a Spark application.
        """
        return pulumi.get(self, "num_executors")

    @num_executors.setter
    def num_executors(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "num_executors", value)

    @_builtins.property
    @pulumi.getter(name="obsBucket")
    def obs_bucket(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        OBS bucket for storing the Spark jobs.
        """
        return pulumi.get(self, "obs_bucket")

    @obs_bucket.setter
    def obs_bucket(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "obs_bucket", value)

    @_builtins.property
    @pulumi.getter(name="pythonFiles")
    def python_files(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]:
        """
        Name of the package that is of the PyFile type and has been uploaded to the DLI resource management system.
        """
        return pulumi.get(self, "python_files")

    @python_files.setter
    def python_files(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[_builtins.str]]]]):
        pulumi.set(self, "python_files", value)

    @_builtins.property
    @pulumi.getter(name="queueName")
    def queue_name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        The DLI queue name.
        """
        return pulumi.get(self, "queue_name")

    @queue_name.setter
    def queue_name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "queue_name", value)

    @_builtins.property
    @pulumi.getter
    def resources(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyResourceArgs']]]]:
        """
        The list of resource objects.
        """
        return pulumi.get(self, "resources")

    @resources.setter
    def resources(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyResourceArgs']]]]):
        pulumi.set(self, "resources", value)

    @_builtins.property
    @pulumi.getter
    def specification(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Compute resource type. Currently, resource types A, B, and C are available.
        """
        return pulumi.get(self, "specification")

    @specification.setter
    def specification(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "specification", value)


if not MYPY:
    class TemplateSparkBodyDependentPackageArgsDict(TypedDict):
        name: NotRequired[pulumi.Input[_builtins.str]]
        """
        User group name.
        """
        resources: NotRequired[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageResourceArgsDict']]]]
        """
        User group resource.
        """
elif False:
    TemplateSparkBodyDependentPackageArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class TemplateSparkBodyDependentPackageArgs:
    def __init__(__self__, *,
                 name: Optional[pulumi.Input[_builtins.str]] = None,
                 resources: Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageResourceArgs']]]] = None):
        """
        :param pulumi.Input[_builtins.str] name: User group name.
        :param pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageResourceArgs']]] resources: User group resource.
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if resources is not None:
            pulumi.set(__self__, "resources", resources)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        User group name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter
    def resources(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageResourceArgs']]]]:
        """
        User group resource.
        """
        return pulumi.get(self, "resources")

    @resources.setter
    def resources(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['TemplateSparkBodyDependentPackageResourceArgs']]]]):
        pulumi.set(self, "resources", value)


if not MYPY:
    class TemplateSparkBodyDependentPackageResourceArgsDict(TypedDict):
        name: NotRequired[pulumi.Input[_builtins.str]]
        """
        Resource name.
        """
        type: NotRequired[pulumi.Input[_builtins.str]]
        """
        Resource type.
        """
elif False:
    TemplateSparkBodyDependentPackageResourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class TemplateSparkBodyDependentPackageResourceArgs:
    def __init__(__self__, *,
                 name: Optional[pulumi.Input[_builtins.str]] = None,
                 type: Optional[pulumi.Input[_builtins.str]] = None):
        """
        :param pulumi.Input[_builtins.str] name: Resource name.
        :param pulumi.Input[_builtins.str] type: Resource type.
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if type is not None:
            pulumi.set(__self__, "type", type)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Resource name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter
    def type(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Resource type.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "type", value)


if not MYPY:
    class TemplateSparkBodyResourceArgsDict(TypedDict):
        name: NotRequired[pulumi.Input[_builtins.str]]
        """
        Resource name.
        """
        type: NotRequired[pulumi.Input[_builtins.str]]
        """
        Resource type.
        """
elif False:
    TemplateSparkBodyResourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class TemplateSparkBodyResourceArgs:
    def __init__(__self__, *,
                 name: Optional[pulumi.Input[_builtins.str]] = None,
                 type: Optional[pulumi.Input[_builtins.str]] = None):
        """
        :param pulumi.Input[_builtins.str] name: Resource name.
        :param pulumi.Input[_builtins.str] type: Resource type.
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if type is not None:
            pulumi.set(__self__, "type", type)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Resource name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "name", value)

    @_builtins.property
    @pulumi.getter
    def type(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Resource type.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "type", value)


